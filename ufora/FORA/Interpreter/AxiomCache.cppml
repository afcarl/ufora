/***************************************************************************
   Copyright 2015 Ufora Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
****************************************************************************/
#include "AxiomCache.hppml"
#include "InterpreterScratchSpace.hpp"
#include "../Axioms/AxiomGroupInterpreterCache.hppml"
#include "../Core/ClassMediator.hppml"
#include "../TypedFora/TypedForaUtil.hppml"
#include "../TypedFora/JitCompiler/TypedJumpTarget.hppml"

namespace Fora {
namespace Interpreter {

SingleAxiomCache::SingleAxiomCache(TypedFora::Compiler* inCompiler, const Axiom& inAxiom) :
		mAxiom(inAxiom),
		mUseCount(0),
		mAxiomEntryBlockSize(0),
		mRuntime(inCompiler),
		mLastLookupPair(0),
		mIsInitialized(false)
	{
	@match Axiom(mAxiom)
		-| Expands(jovt) ->> {
			mLastIsStarArgs = jovt.hasExtras();
			}
		-| Native(jovt) ->> {
			mLastIsStarArgs = jovt.hasExtras();
			}
	}

const Axiom& SingleAxiomCache::getAxiom(void) const
	{
	return mAxiom;
	}

bool SingleAxiomCache::isExpansion()
	{
	return mAxiom.isExpands();
	}

bool SingleAxiomCache::isNativeCall()
	{
	return mAxiom.isNative();
	}

const ControlFlowGraph& SingleAxiomCache::getControlFlowGraph(const ApplySignature& inSignature) const
	{
	pair<const hash_type, ControlFlowGraph>* ptr = mLastLookupPair;

	if (ptr && inSignature.hash() == ptr->first)
		return ptr->second;

	boost::recursive_mutex::scoped_lock lock(mMutex);

	auto it = mGraphs.find(inSignature.hash());
	if (it == mGraphs.end())
		{
		@match Axiom(mAxiom)
			-| Expands(_, graphFunction) ->> {
				mGraphs[inSignature.hash()] = graphFunction(inSignature);
				return mGraphs[inSignature.hash()];
				}
		}
	else
		{
		mLastLookupPair = &*it;
		return it->second;
		}
	}

bool  SingleAxiomCache::lastIsStarArgs() const
	{
	return mLastIsStarArgs;
	}

NativeFunctionPointerAndEntrypointId SingleAxiomCache::getPtr()
	{
	lassert(mAxiom.isNative());

	if (!mIsInitialized)
		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		if (!mIsInitialized)
			{
			TypedFora::Callable targ;
			JOVT jovt;

			@match Axiom(mAxiom)
				-| Native(callSignature,_,callTarget) ->> {
					targ = *callTarget;
					jovt = callSignature;
					};

			targ =
				TypedFora::adaptSignatureOfCallable(
					targ,
					jovt,
					jovt,
					TypedFora::interpreterResultSignature(),
					false
					);

			string axiomName = "Axiom::" + prettyPrintString(jovt);

			mRuntime->define(axiomName, targ);

			TypedFora::TypedJumpTarget jumpTarget =
				mRuntime->getJumpTarget(axiomName, TypedFora::BlockID::entry(), true);

			lassert(!jumpTarget.functionPointer().isEmpty());

			//how are we packing these?
			for (long k = 0; k < jovt.jovs().size(); k++)
				{
				JOV jov = jovt.jovs()[k];
				if (jov.constant())
					mAxiomEntryPackCmds.push_back(cmd_pack_nothing);
					else
				if (jov.type())
					{
					mAxiomEntryBlockSize += jov.type()->size();
					mAxiomEntryPackCmds.push_back(cmd_pack_data);
					}
					else
					{
					mAxiomEntryBlockSize += sizeof(ImplVal);
					mAxiomEntryPackCmds.push_back(cmd_pack_implval);
					}
				}
			if (jovt.extras().isExtras())
				{
				mAxiomEntryBlockSize += sizeof(ImplVal);
				mAxiomEntryPackCmds.push_back(cmd_pack_remainingArgs);
				}

			NativeFunctionPointerAndEntrypointId ptr = jumpTarget.functionPointer().get();

			lassert(!jumpTarget.functionPointer().get().isEmpty());

			mNativeEntryPointer = ptr;

			//because we are reading 'mIsInitialized' outside of the lock, it is possible for
			//other threads to see a write to mIsInitialized before all of the other writes here
			//are visible. In this case, we may see a corrupted version of some of the data structures
			//we have initialized here.  If we always read these under a lock it wouldn't be a problem
			//Instead, force the writes to post, and then write into mIsInitialized
			fullMemoryBarrier();

			mIsInitialized = true;
			}
		}

	return *mNativeEntryPointer;
	}

uword_t SingleAxiomCache::entryDataSize()
	{
	return mAxiomEntryBlockSize;
	}

void SingleAxiomCache::packEntryArguments(
							char* data,
							Fora::ApplyArgFrame& ioImplVals,
							InterpreterScratchSpace& scratchSpace
							)
	{
	uword_t ct = ioImplVals.size();

	for (long k = 0; k < mAxiomEntryPackCmds.size(); k++)
		{
		if (mAxiomEntryPackCmds[k] == cmd_pack_nothing)
			;
			else
		if (mAxiomEntryPackCmds[k] == cmd_pack_data)
			{
			ImplVal val = ioImplVals[k].first;

			uword_t sz = val.type().size();
			memcpy(data, val.data(), sz);
			data += sz;
			}
			else
		if (mAxiomEntryPackCmds[k] == cmd_pack_implval)
			{
			*((ImplVal*)data) = ioImplVals[k].first;
			data += sizeof(ImplVal);
			}
			else
			{
			scratchSpace.loadAxiomSpilloverData(ioImplVals, k);

			*((ImplVal*)data) = ImplVal(scratchSpace.getAxiomSpilloverType(),
										scratchSpace.getAxiomSpilloverData());

			data += sizeof(ImplVal);
			}
		}
	}

SingleAxiomGroupCache::SingleAxiomGroupCache(
								PolymorphicSharedPtr<AxiomGroup> inGroup,
								TypedFora::Compiler* inCompiler
								) :
		mGroup(inGroup),
		mRuntime(inCompiler),
		mCache(inGroup->createInterpreterCache())
	{
	}

SingleAxiomCache* SingleAxiomGroupCache::whichAxiom(
						const Fora::ApplyArgFrame& values
						)
	{
	void* payload = mCache->lookup(values);

	if (!payload)
		{
		JudgmentOnValueTuple jovt = mGroup->weakestAxiomFor(values);

		Nullable<Axiom> axiom = mGroup->bestAxiom(*mRuntime, jovt);
		lassert_dump(axiom, prettyPrintString(jovt)
			<< " didn't generate an axiom even though it was the result of "
			<< "weakestAxiom");

		payload = new SingleAxiomCache(mRuntime, *axiom);

		mCache->addToCache(jovt, payload);
		}

	return (SingleAxiomCache*)payload;
	}


AxiomCache::AxiomCache(
				TypedFora::Compiler* inCompiler,
				PolymorphicSharedPtr<Axioms> inAxioms
				) :
			mRuntime(inCompiler),
			mInterpreterAxioms(inAxioms->interpreterAxioms()),
			mClassGroupIndex(-1),
			mAxioms(inAxioms)
	{
	mUseCount.resize(mInterpreterAxioms.size(), 1);

	for (uword_t k = 0; k < mInterpreterAxioms.size(); k++)
		mGroupCaches.push_back(
			new SingleAxiomGroupCache(mInterpreterAxioms[k].second, mRuntime)
			);
	}

SingleAxiomCache* AxiomCache::whichAxiom(const Fora::ApplyArgFrame& values)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	uword_t groupIndex = 0;

	if (values.size() && values[0].first.type().isClass())
		{
		if (mClassGroupIndex != -1)
			groupIndex = mClassGroupIndex;
		else
			{
			groupIndex = mAxioms->axiomSearchTree(values);
			mClassGroupIndex = groupIndex;
			}
		}
	else
		groupIndex = groupIndex = mAxioms->axiomSearchTree(values);

	mUseCount[groupIndex]++;

	return mGroupCaches[groupIndex]->whichAxiom(values);
	}

TypedFora::Compiler*	AxiomCache::typedForaCompiler(void) const
	{
	return mRuntime;
	}

}
}
