/***************************************************************************
    Copyright 2016 Ufora Inc.
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
 
        http://www.apache.org/licenses/LICENSE-2.0
 
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/

#include "BigVectorId.hppml"
#include "../TypedFora/ABI/BigVectorPageLayout.hppml"
#include "PageId.hppml"
#include "../../core/containers/MapWithIndex.hpp"
#include "../../core/containers/TwoWaySetMap.hpp"
#include "../../core/math/IntegerRange.hppml"
#include "../../core/math/RangeToIntegerSequence.hppml"
#include "../../core/math/IntegerRanges.hppml"

/**************

This class tracks the relationship between blocks of physical memory we've allocated
in shared memory pools, the data we have placed in them, and the mappings we've applied.

This class takes care of several things:
    * we need to keep track of which physical memory holds what data currently
    * we need to be able to tell in what locations the vector data we've already
        placed lives so we can get it back if need be.
    * when new vector data shows up, we need to figure out an ideal place to store it.
        this means looping over the various bigvecs that could hold it and seeing if
        we have storage allocated already for it.
    * when we want to map data for an address, we want to see if we already have storage
        for mappable for that page. if we can map it directly we do it immediately, but we may
        need to make an extra copy of the data.

***************/


@type BigvecOrPageId = 
    -| Bigvec of Fora::BigVectorId bigvec
    -| Page of Fora::PageId page
    ;

//describes a single allocation zone. Zone memory is always page aligned
@type AllocationZone = 
    //raw page data, with first byte of the page starting at 'byteOffset' inside the zone
    -| Page of Fora::PageId page, uint64_t byteOffset
    //a bigvec zone.
    -| Bigvec of Fora::BigVectorId bigvec
    ;

//describes how a visible region of memory is mapped to an AllocationZone
@type RangeToZoneMapping = 
    IntegerRange byteRangeInObject,
    AllocationZone zoneContainingData,
    IntegerRange byteRangeInZone
{
public:
    uint64_t objectByteOffsetToZone(uint64_t o) const
        {
        return o - byteRangeInObject().low() + byteRangeInZone().low();
        }

    RangeToZoneMapping expandedToZoneRange(IntegerRange rangeInZone) const
        {
        RangeToZoneMapping res = *this;

        res.byteRangeInObject().low() += rangeInZone.low() - res.byteRangeInZone().low();
        res.byteRangeInObject().high() += rangeInZone.high() - res.byteRangeInZone().high();

        res.byteRangeInZone() = rangeInZone;

        return res;
        }

    RangeToZoneMapping restrictedToObjectRange(IntegerRange rangeInObject) const
        {
        lassert(byteRangeInObject().contains(rangeInObject));

        return RangeToZoneMapping(
            rangeInObject, 
            zoneContainingData(), 
            byteRangeInZone().slice(rangeInObject - byteRangeInObject().low())
            );
        }

    RangeToZoneMapping restrictedToZoneRange(IntegerRange rangeInZone) const
        {
        lassert_dump(
            byteRangeInZone().contains(rangeInZone),
            "Expected " << prettyPrintString(byteRangeInZone())
                << " to fully contain " << prettyPrintString(rangeInZone)
            );

        return RangeToZoneMapping(
            byteRangeInObject().slice(rangeInZone - byteRangeInZone().low()), 
            zoneContainingData(), 
            rangeInZone
            );
        }
};

//describes how a part of an object is mapped into its original pages
@type ZoneToPageMapping = 
    IntegerRange byteRangeInZone,
    Fora::PageId page,
    IntegerRange byteRangeInPage
{
public:
    IntegerRange mapZoneSubrangeToPage(IntegerRange zoneSubrange) const
        {
        lassert(byteRangeInZone().contains(zoneSubrange));
        return zoneSubrange - byteRangeInZone().low() + byteRangeInPage().low();
        }
};

//represents a group of pages allocated to be the physical backing
//of a bigvec or a page
@type PhysicalMemoryAllocation = 
    AllocationZone zone,
    IntegerRange byteRange
    ;

@type AllocationToPageMapping = 
    IntegerRange byteRangeInAlloc,
    Fora::PageId page,
    IntegerRange byteRangeInPage
    ;

@type DataCopyTask = 
    uint8_t* dest,
    uint8_t* source,
    int64_t bytecount
{
public:
    void copy()
        {
        memcpy(dest(), source(), bytecount());
        }
};

macro_defineCppmlComparisonOperators(BigvecOrPageId);
macro_defineCppmlComparisonOperators(PhysicalMemoryAllocation);
macro_defineCppmlComparisonOperators(AllocationZone);


//decides how we map bigvecs to allocation zones
class AllocationZoneMapping {
public:
    AllocationZoneMapping(int64_t pageSize) : mPageSize(pageSize)
        {
        }

    void discardBigvec(const Fora::BigVectorId& id)
        {
        lassert_dump(false, "not implemented");
        }

    void addBigvec(const TypedFora::Abi::BigVectorPageLayout& layout)
        {
        int64_t elementSize = layout.identity().jor()[0].type()->size();

        map<int64_t, TypedFora::Abi::VectorDataIDSlice>& slices(mBigvecSliceMapping[layout.identity()]);

        IntegerRanges<false>& ranges(mBigvecPageBoundaries[layout.identity()]);

        int64_t lastByteRange = 0;

        long cumulative = 0;
        for (auto slice: layout.vectorIdentities())
            {
            IntegerRange byterange = 
                IntegerRange(cumulative, cumulative + slice.slice().size()) * elementSize;

            //we can map a portion of the Page zone into this one
            //round the low byterange up to pagesize. AllocationZone mappings are always
            //aligned on the page

            uint64_t pageBoundary = roundUpToPageSize(byterange.low());
            uint64_t pageBoundaryTop = roundDownToPageSize(byterange.high());

            if (pageBoundary < pageBoundaryTop)
                {
                if (pageBoundary > lastByteRange)
                    {
                    addBigvecZone(
                        layout.identity(),
                        RangeToZoneMapping(
                            IntegerRange(lastByteRange, pageBoundary),
                            AllocationZone::Bigvec(layout.identity()),
                            IntegerRange(lastByteRange, pageBoundary)
                            )
                        );
                    }

                uint64_t firstValueOfMappingByteOffsetInPage = slice.slice().low() * elementSize;
                
                uint64_t pageBoundaryByteOffsetInPage = 
                    firstValueOfMappingByteOffsetInPage + (pageBoundary - byterange.low());

                //we need to pick a byte offset for the first byte in the page so that
                //    pageBoundaryByteOffsetInPage + pageByteOffset 
                //is page aligned

                //this is the 'byte' that the first piece of page data will have to 
                //reside in in the AllocationZone if we're going to use this page.
                uint64_t pageByteOffset = roundUpToPageSize(pageBoundaryByteOffsetInPage) - pageBoundaryByteOffsetInPage;

                lassert(pageByteOffset >= 0 && pageByteOffset < mPageSize);

                //this is the first byte of page data that 'pageBoundary' will represent
                uint64_t firstByteOfPageDataReferencedByRange = 
                    pageByteOffset + pageBoundaryByteOffsetInPage;

                lassert(firstByteOfPageDataReferencedByRange % mPageSize == 0);

                addBigvecZone(
                    layout.identity(),
                    RangeToZoneMapping(
                        IntegerRange(pageBoundary, pageBoundaryTop),
                        AllocationZone::Page(slice.vector().getPage(), pageByteOffset),
                        IntegerRange(
                            firstByteOfPageDataReferencedByRange,
                            firstByteOfPageDataReferencedByRange + 
                                pageBoundaryTop - pageBoundary
                            )
                        )
                    );

                lastByteRange = pageBoundaryTop;
                }

            ranges.addRange(byterange);

            slices[cumulative] = slice;
            cumulative += slice.slice().size();
            }

        if (cumulative > lastByteRange)
            {
            int64_t roundedUp = roundUpToPageSize(cumulative);

            addBigvecZone(
                layout.identity(),
                RangeToZoneMapping(
                    IntegerRange(lastByteRange, roundedUp),
                    AllocationZone::Bigvec(layout.identity()),
                    IntegerRange(lastByteRange, roundedUp)
                    )
                );
            }
        }

    IntegerRange zeroPaddingBytesFor(PhysicalMemoryAllocation alloc)
        {
        if (alloc.zone().isPage())
            return IntegerRange();

        auto bigvec = alloc.zone().getBigvec().bigvec();

        int64_t elementSize = bigvec.jor()[0].type()->size();

        int64_t bytecount = bigvec.size() * elementSize;

        IntegerRange byteRange = alloc.byteRange();

        if (byteRange.high() > bytecount)
            {
            byteRange.low() = bytecount;
            byteRange = byteRange - alloc.byteRange().low();
            }
        else
            byteRange = IntegerRange();

        return byteRange;
        }

    RangeToZoneMapping zoneMappingFor(const BigvecOrPageId& object, IntegerRange page) const
        {
        lassert_dump(page == clipToPageSize(page), "range argument is intended to be a single page");

        @match BigvecOrPageId(object)
            -| Bigvec(id) ->> {
                auto it = mBigvecZoneBoundaries.find(id);
                lassert(it != mBigvecZoneBoundaries.end());

                Nullable<IntegerRange> nRng = it->second.rangeContaining(page.low());

                lassert_dump(nRng, "can't find " << page.low() << " for bigvec " 
                    << prettyPrintString(id) << " which has slices "
                    << prettyPrintString(mBigvecSliceMapping.find(id)->second)
                    );


                IntegerRange rng = *nRng;

                auto it2 = mBigvecZoneMappings.find(make_pair(id, rng));

                lassert(it2 != mBigvecZoneMappings.end());

                return it2->second.restrictedToObjectRange(page);
                }
            -| Page(p) ->> {
                return RangeToZoneMapping(page, AllocationZone::Page(p,0), page);
                }
        }

    const std::map<AllocationZone, long>& zonesActiveForPage(const Fora::PageId& page)
        {
        //ensure the base zone is populated
        mPageZoneRefcounts[page][AllocationZone::Page(page, 0)];

        return mPageZoneRefcounts[page];
        }

    void mapZoneRangeToPageRanges(
                AllocationZone zone, 
                IntegerRange byteRange, 
                std::vector<ZoneToPageMapping>& outRanges,
                IntegerRange& outBytesOfPaddingZeros
                )
        {
        outBytesOfPaddingZeros = IntegerRange();

        @match AllocationZone(zone)
            -| Page(p, byteOffset) ->> {
                outRanges.push_back(ZoneToPageMapping(byteRange, p, byteRange - byteOffset));
                }
            -| Bigvec(bigvec) ->> {
                int64_t elementSize = bigvec.jor()[0].type()->size();

                std::vector<IntegerRange> ranges;
                
                mBigvecPageBoundaries[bigvec].rangesIntersecting(byteRange, ranges);

                for (auto rng: ranges)
                    {
                    TypedFora::Abi::VectorDataIDSlice slice = mBigvecSliceMapping[bigvec][rng.low()];

                    //this particular slice is this byte offset within the page
                    IntegerRange rangeOfSliceWithinPage = 
                        IntegerRange(0, elementSize * slice.slice().size()) + 
                            elementSize * slice.slice().low()
                            ;

                    //this is the active range within the slice
                    IntegerRange rangeWithinSlice = rng.intersect(byteRange) - rng.low();

                    lassert(rangeWithinSlice.size() <= rangeOfSliceWithinPage.size());

                    IntegerRange valuesWithinPage = rangeWithinSlice + rangeOfSliceWithinPage.low();

                    lassert(valuesWithinPage.low() >= 0);

                    outRanges.push_back(
                        ZoneToPageMapping(
                            rng.intersect(byteRange), 
                            slice.vector().getPage(), 
                            valuesWithinPage
                            )
                        );
                    }

                if (byteRange.high() > bigvec.size() * elementSize)
                    outBytesOfPaddingZeros = IntegerRange(
                        bigvec.size() * elementSize,
                        byteRange.high()
                        );
                }

        int64_t totalBytes = outBytesOfPaddingZeros.size();
        
        for (auto mapping: outRanges)
            totalBytes += mapping.byteRangeInPage().size();

        lassert(totalBytes == byteRange.size());
        }

    uint64_t roundUpToPageSize(uint64_t o) const
        {
        auto frac = o % mPageSize;
        if (frac)
            return o + mPageSize - frac;
        return o;
        }

    uint64_t roundDownToPageSize(uint64_t o) const
        {
        return o - o % mPageSize;
        }

    IntegerRange clipToPageSize(IntegerRange in) const
        {
        in.low() = roundUpToPageSize(in.low());
        in.high() = roundDownToPageSize(in.high());
        return in;
        }

    int64_t pageSize() const
        {
        return mPageSize;
        }

private:
    void addBigvecZone(Fora::BigVectorId id, RangeToZoneMapping mapping)
        {
        mBigvecZoneBoundaries[id].addRange(mapping.byteRangeInObject());
        mBigvecZoneMappings[make_pair(id,mapping.byteRangeInObject())] = mapping;

        @match AllocationZone(mapping.zoneContainingData())
            -| Page(p) ->> {
                mPageZoneRefcounts[p][mapping.zoneContainingData()]++;
                }
            -| _ ->> {}
        }

    int64_t mPageSize;

    map<Fora::BigVectorId, map<int64_t, TypedFora::Abi::VectorDataIDSlice> > mBigvecSliceMapping;

    map<Fora::BigVectorId, IntegerRanges<false> > mBigvecPageBoundaries;

    map<Fora::BigVectorId, IntegerRanges<false> > mBigvecZoneBoundaries;

    map<pair<Fora::BigVectorId, IntegerRange>, RangeToZoneMapping> mBigvecZoneMappings;

    std::map<Fora::PageId, std::map<AllocationZone, long> > mPageZoneRefcounts; 
};

class PhysicalAllocations {
public:
    PhysicalAllocations(
                boost::function<uint8_t* (PhysicalMemoryAllocation alloc)> inAllocator,
                boost::function<void (PhysicalMemoryAllocation alloc, uint8_t*)> inDeallocator
                ) : 
            mAllocateFunc(inAllocator),
            mDeallocateFunc(inDeallocator),
            mTotalBytesMapped(0),
            mIncreasingIndex(0)
        {
        }

    Nullable<PhysicalMemoryAllocation> allocForAddress(uint8_t* addr) const
        {
        auto nRng = mPhysicalMemoryAddressesAsRanges.rangeContaining((int64_t)addr);
        if (!nRng)
            return null();

        const auto& keys = mPhysicalMemoryAddresses.getKeys((uint8_t*)nRng->low());
        lassert(keys.size() == 1);

        return null() << *keys.begin();
        }

    IntegerRanges<false>& allocationsForZone(AllocationZone zone)
        {
        return mZoneBytecountRangesPhysicallyAllocated[zone];
        }

    uint8_t* addressForAllocation(PhysicalMemoryAllocation alloc)
        {
        return mPhysicalMemoryAddresses.getValue(alloc);
        }

    int64_t countOfBigvecAllocs() const
        {
        return mBigvecAllocationOrder.size();
        }

    Nullable<PhysicalMemoryAllocation> oldestBigvecAlloc()
        {
        if (!mBigvecAllocationOrder.size())
            return null();

        const auto& keys = mBigvecAllocationOrder.getKeys(mBigvecAllocationOrder.lowestValue());
        lassert(keys.size() == 1);

        return null() << *keys.begin();
        }

    Nullable<Fora::PageId> getOldestPage()
        {
        if (!mPageZoneAllocations.size())
            return null();

        const auto& keys = mPageZoneAllocations.getKeys(mPageZoneAllocations.lowestValue());
        lassert(keys.size() == 1);

        return null() << *keys.begin();
        }

    void replacePageOnTopOfQueue(Fora::PageId page)
        {
        mPageZoneAllocations.set(page, mIncreasingIndex++);
        }

    pair<PhysicalMemoryAllocation, int64_t> allocContaining(uint8_t* addr)
        {
        IntegerRange addressRange = *mPhysicalMemoryAddressesAsRanges.rangeContaining((int64_t)addr);

        PhysicalMemoryAllocation alloc = *mPhysicalMemoryAddresses.getKeys((uint8_t*)addressRange.low()).begin();

        return pair<PhysicalMemoryAllocation, int64_t>(alloc, addr - (uint8_t*)addressRange.low());
        }

    void allocatePhysicalMemory(PhysicalMemoryAllocation allocation)
        {
        if (allocation.zone().isBigvec())
            mBigvecAllocationOrder.set(allocation, mIncreasingIndex++);
        else
            mPageZoneAllocations.set(allocation.zone().getPage().page(), mIncreasingIndex++);
        
        mTotalBytesMapped += allocation.byteRange().size();

        uint8_t* actualAddress = mAllocateFunc(allocation);

        lassert(actualAddress);

        mPhysicalMemoryAddresses.set(allocation, actualAddress);

        mPhysicalMemoryAddressesAsRanges.addRange(
            IntegerRange((int64_t)actualAddress, (int64_t)actualAddress + allocation.byteRange().size())
            );

        mZoneBytecountRangesPhysicallyAllocated[allocation.zone()].addRange(allocation.byteRange());
        }

    void deallocatePhysicalMemory(PhysicalMemoryAllocation allocation)
        {
        if (allocation.zone().isBigvec())
            mBigvecAllocationOrder.discard(allocation);

        mTotalBytesMapped -= allocation.byteRange().size();
        
        uint8_t* actualAddress = mPhysicalMemoryAddresses.getValue(allocation);

        mDeallocateFunc(allocation, actualAddress);

        mPhysicalMemoryAddresses.discard(allocation);

        mPhysicalMemoryAddressesAsRanges.removeRange(
            IntegerRange((int64_t)actualAddress, (int64_t)actualAddress + allocation.byteRange().size())
            );

        mZoneBytecountRangesPhysicallyAllocated[allocation.zone()].removeRange(allocation.byteRange());
        }

    void setMappableAddress(
                BigvecOrPageId target, 
                uint8_t* address, 
                uint64_t bytecount
                )
        {
        mMappingAddresses.set(target, address);
        }

    uint8_t* getMappableAddress(BigvecOrPageId target)
        {
        if (mMappingAddresses.hasKey(target))
            return mMappingAddresses.getValue(target);

        return nullptr;
        }

    void dropMappingTarget(BigvecOrPageId target)
        {
        mMappingAddresses.discard(target);
        }

    Nullable<pair<BigvecOrPageId, int64_t> > 
            translateMappingAddress(uint8_t* mappingBase, uint64_t offsetInMapping)
        {
        if (mMappingAddresses.size() == 0)
            return null();

        if (mMappingAddresses.getKeys(mappingBase).size() == 0)
            return null();

        lassert(mMappingAddresses.getKeys(mappingBase).size() == 1);

        return null() << make_pair(*mMappingAddresses.getKeys(mappingBase).begin(), (int64_t)offsetInMapping);
        }

    int64_t totalBytesMapped() const
        {
        return mTotalBytesMapped;
        }

    bool allocHasCopyTasks(PhysicalMemoryAllocation alloc) const
        {
        return mPhysicalAllocCopyTaskCount.find(alloc) != mPhysicalAllocCopyTaskCount.begin();
        }

    void copyAddressRefcountChange(uint8_t* addr, int64_t direction)
        {
        Nullable<PhysicalMemoryAllocation> alloc = allocForAddress(addr);
        
        if (alloc)
            {
            mPhysicalAllocCopyTaskCount[*alloc] += direction;
            if (mPhysicalAllocCopyTaskCount[*alloc] == 0)
                mPhysicalAllocCopyTaskCount.erase(*alloc);
            }
        }

private:
    int64_t mTotalBytesMapped;

    int64_t mIncreasingIndex;

    map<PhysicalMemoryAllocation, int64_t> mPhysicalAllocCopyTaskCount;

    MapWithIndex<PhysicalMemoryAllocation, int64_t> mBigvecAllocationOrder;

    MapWithIndex<Fora::PageId, int64_t> mPageZoneAllocations;

    map<AllocationZone, IntegerRanges<false> > mZoneBytecountRangesPhysicallyAllocated;

    MapWithIndex<PhysicalMemoryAllocation, uint8_t*> mPhysicalMemoryAddresses;

    MapWithIndex<BigvecOrPageId, uint8_t*> mMappingAddresses;

    IntegerRanges<false> mMappingAddressesAsRanges;

    IntegerRanges<false> mPhysicalMemoryAddressesAsRanges;

    boost::function<uint8_t* (PhysicalMemoryAllocation alloc)> mAllocateFunc;

    boost::function<void (PhysicalMemoryAllocation alloc, uint8_t*)> mDeallocateFunc;
};


class MemoryMappings {
public:
    MemoryMappings(
                AllocationZoneMapping& inZoneMappings,
                PhysicalAllocations& inPhysicalAllocations,
                boost::function<bool (uint8_t* sharedBase, uint64_t sharedOffset, uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount)> inMapper,
                boost::function<bool (uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount)> inUnmapper
                ) :
            mMapper(inMapper),
            mUnmapper(inUnmapper),
            mZoneMappings(inZoneMappings),
            mPhysicalAllocations(inPhysicalAllocations)
        {
        }

    void removeAllMappingsFor(PhysicalMemoryAllocation alloc)
        {
        for (auto objectAndRange: mMappings[alloc])
            {
            mUnmapper(
                mPhysicalAllocations.getMappableAddress(objectAndRange.first),
                objectAndRange.second.low(),
                objectAndRange.second.size()
                );
            mBytesMapped[objectAndRange.first].removeRange(objectAndRange.second);
            }

        mMappings.erase(alloc);
        }

    void blockedThreadExists(
                BigvecOrPageId object, 
                RangeToZoneMapping zoneMapping
                )
        {
        lassert(zoneMapping.byteRangeInObject().size() == mZoneMappings.pageSize());

        //see if this page is new or not
        if (mZoneMappableRegions[zoneMapping.zoneContainingData()].rangeContaining(zoneMapping.byteRangeInZone().low()))
            {
            LOG_DEBUG << zoneMapping << " looks immediately mappable.";

            //we can map it
            ensureRangeMapped(object, zoneMapping);
            }
        else
            {
            LOG_DEBUG << zoneMapping << " not mappable: " << 
                mZoneMappableRegions[zoneMapping.zoneContainingData()].getRangeStartsAndEnds();

            mPendingMappings[zoneMapping.zoneContainingData()].push_back(make_pair(object, zoneMapping));
            }
        }

    void zoneNeeds(AllocationZone zone, IntegerRange byteRangeInZone, const std::vector<pair<Fora::PageId, IntegerRange> >& inNeededPageData)
        {  
        lassert(inNeededPageData.size());
        lassert(mZonesAwaitingPageData.find(make_pair(zone,byteRangeInZone)) == mZonesAwaitingPageData.end());

        for (auto pageAndRange: inNeededPageData)
            {
            mZonesAwaitingPageData[make_pair(zone,byteRangeInZone)][pageAndRange.first].addRange(pageAndRange.second);
            mZonesAwaitingPageDataPages.insert(make_pair(zone,byteRangeInZone), pageAndRange.first);
            }
        }

    void pageDataProvided(Fora::PageId page, IntegerRange range)
        {
        std::set<pair<AllocationZone, IntegerRange> > toCheck = mZonesAwaitingPageDataPages.getKeys(page);

        for (auto& zoneAndRange: toCheck)
            {
            mZonesAwaitingPageData[zoneAndRange][page].removeRange(range);
            
            if (mZonesAwaitingPageData[zoneAndRange][page].isEmpty())
                {
                mZonesAwaitingPageData[zoneAndRange].erase(page);
                mZonesAwaitingPageDataPages.drop(zoneAndRange, page);
                }

            if (mZonesAwaitingPageData[zoneAndRange].size() == 0)
                {
                mZonesToTryToMap.push_back(zoneAndRange);
                mZonesAwaitingPageData.erase(zoneAndRange);   
                }
            }
        }

    void zoneRangeNotMappable(AllocationZone zone, IntegerRange range)
        {
        mZoneMappableRegions[zone].removeRange(range);
        }

    void zoneIsNowMappable(AllocationZone zone, IntegerRange range)
        {
        mZoneMappableRegions[zone].addRange(range);

        LOG_DEBUG << zone << " added " 
            << range << ": " << mZoneMappableRegions[zone].getRangeStartsAndEnds();

        std::vector<pair<BigvecOrPageId, RangeToZoneMapping> >& mappings = mPendingMappings[zone];

        for (long k = 0; k < mappings.size(); k++)
            if (mZoneMappableRegions[zone].completelyContainsRange(mappings[k].second.byteRangeInZone()))
                {
                ensureRangeMapped(mappings[k].first, mappings[k].second);
                mappings.erase(mappings.begin() + k);
                k--;
                }

        if (mappings.size() == 0)
            mPendingMappings.erase(zone);
        }

    void extractZonesToTryToMap(std::vector<pair<AllocationZone, IntegerRange> >& outZones)
        {
        outZones.clear();
        std::swap(outZones, mZonesToTryToMap);
        }

    bool hasZonesToTryToMap() const
        {
        return mZonesToTryToMap.size();
        }

    bool byteIsMapped(BigvecOrPageId object, int64_t byteOffset)
        {
        return mBytesMapped[object].contains(byteOffset);
        }

private:
    void ensureRangeMapped(BigvecOrPageId object, RangeToZoneMapping mapping)
        {
        IntegerRange zoneInObject = mapping.byteRangeInObject();

        std::vector<IntegerRange> bytesInObjectToMap;

        mBytesMapped[object].rangesNotCovered(zoneInObject, bytesInObjectToMap);

        for (auto range: bytesInObjectToMap)
            {
            mBytesMapped[object].addRange(range);

            mapByteRange(object, mapping.restrictedToObjectRange(range));
            }
        }

    void mapByteRange(BigvecOrPageId object, RangeToZoneMapping mapping)
        {
        std::vector<IntegerRange> physicalRanges;

        mPhysicalAllocations.allocationsForZone(mapping.zoneContainingData())
            .rangesIntersecting(mapping.byteRangeInZone(), physicalRanges);

        for (auto range: physicalRanges)
            {
            auto restricted = mapping.restrictedToZoneRange(range.intersect(mapping.byteRangeInZone()));

            mapByteRange(
                object, 
                restricted.byteRangeInObject(),
                PhysicalMemoryAllocation(mapping.zoneContainingData(), range),
                restricted.byteRangeInZone() - range.low()
                );
            }
        }

    void mapByteRange(BigvecOrPageId object, IntegerRange rangeInObject, PhysicalMemoryAllocation alloc, IntegerRange rangeInAlloc)
        {
        lassert(rangeInObject.size() == rangeInAlloc.size());

        mMapper(
            mPhysicalAllocations.addressForAllocation(alloc),
            rangeInAlloc.low(),
            mPhysicalAllocations.getMappableAddress(object),
            rangeInObject.low(),
            rangeInObject.size()
            );

        mMappings[alloc].push_back(make_pair(object, rangeInObject));
        }

    AllocationZoneMapping& mZoneMappings;

    PhysicalAllocations& mPhysicalAllocations;

    boost::function<bool (uint8_t* sharedBase, uint64_t sharedOffset, uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount)> mMapper;

    boost::function<bool (uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount)> mUnmapper;

    //state machine for zones that want to wake up when page data is provided from the outside
    std::map<pair<AllocationZone, IntegerRange>, std::map<Fora::PageId, IntegerRanges<true> > >  mZonesAwaitingPageData;

    TwoWaySetMap<pair<AllocationZone, IntegerRange>, Fora::PageId> mZonesAwaitingPageDataPages;

    std::vector<pair<AllocationZone, IntegerRange> > mZonesToTryToMap;

    std::map<BigvecOrPageId, IntegerRanges<true> > mBytesMapped;

    std::map<AllocationZone, IntegerRanges<true> > mZoneMappableRegions;

    std::map<AllocationZone, std::vector<pair<BigvecOrPageId, RangeToZoneMapping> > > mPendingMappings;

    std::map<PhysicalMemoryAllocation, std::vector<pair<BigvecOrPageId, IntegerRange> > > mMappings;
};

class PageDataPlacements {
public:
    PageDataPlacements(
                int64_t inPageSize,
                boost::function<uint8_t* (PhysicalMemoryAllocation alloc)> inAllocator,
                boost::function<void (PhysicalMemoryAllocation alloc, uint8_t*)> inDeallocator,
                boost::function<bool (uint8_t* sharedBase, uint64_t sharedOffset, uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount)> inMapper,
                boost::function<bool (uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount)> inUnmapper,
                boost::function<void (Fora::PageId page, IntegerRange bytecountRange)> inBroadcaster,
                int64_t maxExcessBytes,
                int64_t maxBigvecAllocations
                );

    //is this object/element offset validly mapped?
    bool byteIsMapped(BigvecOrPageId mapping, int64_t byteOffset);

    //indicate that a thread is blocked reading this data. If the value is already mapped,
    //we disregard it.
    void markThreadBlockedOn(BigvecOrPageId mapping, int64_t byteOffset);

    void tryToConstructZone(AllocationZone zone, IntegerRange byteRangeInZone);

    //given a mapping and a byte offset in the mapping, what is the object and relative byte-offset it corresponds to?
    Nullable<pair<BigvecOrPageId, int64_t> > translateMappingAddress(uint8_t* mappingBase, uint64_t offsetInMapping);

    void addBigVecPageLayout(const TypedFora::Abi::BigVectorPageLayout& layout);

    //we've produced a memory-mapping to 'target'
    void setMappableAddress(
                BigvecOrPageId target, 
                uint8_t* address, 
                uint64_t bytecount
                );

    //get the base address for a given target, or nullptr if none exists
    uint8_t* getMappableAddress(BigvecOrPageId target);

    uint8_t* getWriteableAddress(PhysicalMemoryAllocation allocation);

    //drop the mapping target and mark all physical memory that's mapped to it as unmapped.
    void dropMappingTarget(BigvecOrPageId target);

    //indicates that some requested data was copied
    void taskComplete(DataCopyTask task);

    //get a list of all of the page placements
    void dropAllDataForPage(const Fora::PageId& inPage);

    Nullable<DataCopyTask> extractTask();

    //add some new data from the outside. Clients are expected to 
    //flush all outstanding memory copy tasks from the system before 
    //addr may be released.
    bool allocatePageData(
                    const Fora::PageId& inPage, 
                    IntegerRange byteRange,
                    uint8_t* addr
                    );

private:
    //after all copy and block operations, we call this function, which synchronously schedules any data
    void releaseMemoryIfNecessary();

    void releaseBigvecsIfNecessary();

    void consolidatePagesIfNecessary();

    //we wrote some page data into a block of physical memory and need to remember that that
    //data is populated
    void pageDataPlacedInPhysicalMemory(Fora::PageId page, IntegerSequence bytesWithinPage, PhysicalMemoryAllocation allocation, IntegerRange bytesWithinPhysical);

    void ensurePhysicalBackingFor(
                const AllocationZone& inZone, 
                IntegerRange bytes
                );
    
    void scheduleNecessaryTasksForMapping(AllocationZone zone, IntegerRange byteRangeInZone);
    
    void scheduleTask(DataCopyTask task);

    IntegerRange pageContainingByteOffset(int64_t byteOffset);

    IntegerRange expandRangeToPageSize(IntegerRange byteRange, int64_t pagesPerBlock);

    //populate the data for 'AllocationZone' from some data somewhere
    void scheduleCopyTask(PhysicalMemoryAllocation zone, AllocationToPageMapping zonePageMapping);

    //populate the data for 'AllocationZone' from some data somewhere
    void scheduleCopyTask(AllocationZone zone, ZoneToPageMapping zonePageMapping);

    int64_t mMaxExcessBytes;

    int64_t mMaxBigvecAllocations;

    int64_t mPageSize;

    //for each page, what values do we have actual definitions for somewhere
    map<Fora::PageId, IntegerRanges<true> > mPageDataBytecountRangesDefined;

    map<Fora::PageId, IntegerRanges<true> > mPageDataBytecountRangesDefinedOrRequested;

    //for each allocation, which ranges actually have values copied in?
    map<PhysicalMemoryAllocation, IntegerRanges<true> > mPhysicalMemoryBytecountRangesPopulated;

    map<AllocationZone, IntegerRanges<true> > mZoneBytecountRangesPopulated;

    std::deque<DataCopyTask> mTasks;

    boost::function<void (Fora::PageId page, IntegerRange bytecountRange)> mBroadcastFunc;

    AllocationZoneMapping mAllocationZoneMapping;

    PhysicalAllocations mPhysicalAllocations;

    MemoryMappings mMemoryMappings;
};
