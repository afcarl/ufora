/***************************************************************************
    Copyright 2015 Ufora Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/

#include "CUDAExecutionContext.hppml"
#include "CUDAVectorRecord.hpp"
#include "NativeCFGToPTX.hppml"
#include "../Core/Alignment.hpp"
#include "../Core/AlignmentManager.hpp"
#include "../Core/ExecutionContext.hppml"
#include "../Core/MemoryPool.hpp"
#include "../Core/ImplValContainerUtilities.hppml"
#include "../Native/NativeCFGTransforms/ConvertForGPUExecution.hppml"
#include "../Native/NativeCFGTransforms/Transforms.hppml"
#include "../Native/NativeCode.hppml"
#include "../Native/NativeType.hppml"
#include "../Native/NativeTypeFor.hpp"
#include "../Runtime.hppml"
#include "../TypedFora/JitCompiler/StaticInliner.hppml"
#include "../TypedFora/Converter.hppml"
#include "../TypedFora/ABI/NativeLayoutType.hppml"
#include "../TypedFora/ABI/SlottedForaValueArrayAppend.hppml"
#include "../TypedFora/ABI/VectorRecord.hpp"
#include "../TypedFora/ABI/VectorRecordCodegen.hppml"
#include "../TypedFora/ABI/VectorHandle.hpp"
#include "../TypedFora/TypedFora.hppml"
#include "../Reasoner/SimpleForwardReasoner.hppml"
#include "../VectorDataManager/PageletTree.hppml"
#include "../../core/Logging.hpp"
#include "../../core/threading/Queue.hpp"

#include <cuda.h>
#include <cuda_runtime_api.h>

#include <fstream>
#include <iostream>


using TypedFora::Abi::VectorHandle;
using TypedFora::Abi::VectorRecord;
using Fora::Interpreter::ExecutionContext;

constexpr uint8_t GPU_EXCEPTION_INDEX = 0xFF;

NativeType cudaNativeTypeFor(const Type& type)
	{
	static NativeType vec = NativeTypeFor<TypedFora::Abi::VectorRecord>::get();
	auto res = TypedFora::Abi::nativeLayoutType(type);
	if (res == vec)
		{
		return NativeTypeFor<CUDAVectorRecord>::get();
		}
	else
		{
		return res;
		}
	}

class CUDAExecutionContextInternalState {
public:

	CUDAExecutionContextInternalState() : mDeviceCount(0)
		{
		cudaDriverInitializer();
		cudaDeviceInitializer();
		}


	//initialize CUDA. single threaded, only happens once
	static	void	cudaDriverInitializer()
		{
		static boost::recursive_mutex	mMutex;

		static bool isInitialized = false;

		if (!isInitialized)
			{
			boost::recursive_mutex::scoped_lock	lock(mMutex);
			if (!isInitialized)
				{
				//initialize the driver
				CUresult error = cuInit(0);
				throwOnError(error, "Initialize cuda");
				isInitialized = true;
				}
			}
		}

	static void throwOnError(CUresult error, std::string op)
		{
		if (error != CUDA_SUCCESS)
			{
			const char* ptr = 0;
			cuGetErrorString(error, &ptr);
			throw UnableToConvertToPTX("Couldn't " + op + ": " + std::string(ptr ? ptr : "") +
				". code=" + boost::lexical_cast<string>(error));
			}
		}

	void	cudaDeviceInitializer()
		{
		CUresult error = cuDeviceGetCount(&mDeviceCount);
		throwOnError(error, "Get a device count");

		lassert_dump(mDeviceCount, "there are no CUDA-enabled devices");

		//take the first device
		mCuDevices.resize(mDeviceCount);
		mCuContexts.resize(mDeviceCount);

		for (long devID = 0; devID < mDeviceCount; devID++)
			{
			int major, minor;
			constexpr int DEVICE_NAME_MAX_LENGTH = 256;
			char deviceName[DEVICE_NAME_MAX_LENGTH];

			throwOnError(
				cuDeviceComputeCapability(&major, &minor, devID),
				"get CUDA device compute capability"
				);
			throwOnError(
				cuDeviceGetName(deviceName, DEVICE_NAME_MAX_LENGTH, devID),
				"get CUDA device name"
				);

			LOG_INFO 	<< "CUDA Using Device " << devID
						<< ": \"" << deviceName
						<< "\" with Compute " << major << "." << minor
						<< "capability.\n"
						;

			// pick up device with zero ordinal (default, or devID)
			throwOnError(
				cuDeviceGet(&mCuDevices[devID], devID),
				"get CUDA device handle"
				);

			// Create context
			throwOnError(
				cuCtxCreate(&mCuContexts[devID], 0, mCuDevices[devID]),
				"create CUDA execution context for device"
				);
			}

		for (auto c: mCuContexts)
			mUnusedContexts.write(c);
		}

	string addLineNumbers(const string& str)
		{
		ostringstream outStr;

		uint32_t lastIx = 0;
		uint32_t lineNo = 1;

		for (long k = 0; k < str.size();k++)
			if (str[k] == '\n' || k == str.size() - 1)
				{
				outStr << lineNo << "   ";
				lineNo++;

				outStr << str.substr(lastIx, k - lastIx) << "\n";
				lastIx = k+1;
				}

		return outStr.str();
		}
	void addCudaModule(	const string& inModuleName,
						const string& inFunctionName,
						const string& inPTXCode)
		{
		for (auto context: mCuContexts)
			{
			throwOnError(
				cuCtxSetCurrent(context),
				"set CUDA execution context"
				);

			const unsigned int jitNumOptions = 5;
			CUjit_option *jitOptions = new CUjit_option[jitNumOptions];
			void **jitOptVals = new void*[jitNumOptions];

			int bufferSize = 1024;
			char	jitLogBuffer[1024];
			char	jitErrorBuffer[1024];

			jitOptions[0] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
			jitOptVals[0] = (void *)(size_t)bufferSize;
			jitOptions[1] = CU_JIT_INFO_LOG_BUFFER;
			jitOptVals[1] = jitLogBuffer;

			jitOptions[2] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
			jitOptVals[2] = (void*)(size_t)bufferSize;
			jitOptions[3] = CU_JIT_ERROR_LOG_BUFFER;
			jitOptVals[3] = jitErrorBuffer;

			jitOptions[4] = CU_JIT_MAX_REGISTERS;
			int jitRegCount = 32;
			jitOptVals[4] = (void *)(size_t)jitRegCount;

			CUmodule	cuModule;

			CUresult error = cuModuleLoadDataEx(&cuModule, inPTXCode.c_str(),
							jitNumOptions, jitOptions, (void **)jitOptVals);

			if (error != CUDA_SUCCESS)
				{
				LOG_WARN << "CUDA FAILED TO COMPILE: " +
												string(jitLogBuffer) + "\n" +
												string(jitErrorBuffer) + "\n" +
												"\n\nPTX code = \n\n" +
												addLineNumbers(inPTXCode);

				lassert_dump(false, "CUDA FAILED TO COMPILE: " +
												string(jitLogBuffer) + "\n" +
												string(jitErrorBuffer) + "\n" +
												"\n\nPTX code = \n\n" +
												addLineNumbers(inPTXCode)
												);
				}

			CUfunction cuFun;

			// Get function handle from module
			throwOnError(
				cuModuleGetFunction(&cuFun, cuModule, inFunctionName.c_str()),
				std::string("get function '") + inFunctionName + "' in CUDA Module"
				);

				{
				boost::mutex::scoped_lock lock(mMutex);
				mCuFunctionsByName[make_pair(context, inModuleName)] = cuFun;
				}
			}

		}


	CUfunction functionNameForModule(CUcontext context, std::string inModuleName)
		{
		boost::mutex::scoped_lock lock(mMutex);
		return mCuFunctionsByName[make_pair(context, inModuleName)];
		}

	void executeFunction(	const std::string&				inModuleName,
							uint8_t*						inCudaClosureData,
							uword_t							inN,
							uint8_t*						inCudaSourceData,
							std::vector<uint8_t*>&			outCudaDestDataVectors,
							uint8_t*						outCudaIndexData
							)
		{
		lassert(outCudaDestDataVectors.size() > 0);

		uword_t outputTypesCount = outCudaDestDataVectors.size();
		LOG_DEBUG << "executeFunction : 1: outputTypesCount = " << outputTypesCount;

		CUcontext contextToUse = mUnusedContexts.get();

		try {
			throwOnError(
				cuCtxSetCurrent(contextToUse),
				"set CUDA execution context"
				);
			std::vector<CUdeviceptr> d_destData(outputTypesCount);

			CUresult error;

			// Grid/Block configuration
			int threadsPerBlock = 1024;
			int blocksPerGrid   = (inN + threadsPerBlock - 1) / threadsPerBlock;
			LOG_DEBUG << "executeFunction : 2:";

			uword_t kk = 0;
			uword_t extraArgs = 2; // inN and sourceDataPtr
			if (inCudaClosureData)
				++extraArgs;
			if (outCudaIndexData)
				++extraArgs;
			uword_t argCount = outputTypesCount + extraArgs;
			LOG_DEBUG << "executeFunction : 3: argCount " << argCount;
			void **args = (void**) malloc(sizeof(void*) * (argCount));
			lassert(args);
			if (inCudaClosureData)
				args[kk++] = &inCudaClosureData;
			args[kk++] = &inN;
			args[kk++] = &inCudaSourceData;
			if (outCudaIndexData)
				args[kk++] = &outCudaIndexData;
			for (uword_t k = 0; k < outputTypesCount; ++k)
				{
				lassert(kk < argCount);
				args[kk++] = &outCudaDestDataVectors[k];
				}
			lassert(kk == argCount);

			// Launch the CUDA kernel
			LOG_DEBUG << "executeFunction: 4: About to launch CUDA kernel";

			error = cuLaunchKernel( functionNameForModule(contextToUse, inModuleName),
									blocksPerGrid, 1, 1,
									threadsPerBlock, 1, 1,
									0,
									NULL, args, NULL);
			throwOnError(error, "Launch Kernel: ");
			lassert_dump(error == CUDA_SUCCESS, "UNKNOWN CUDA ERROR");
			cudaDeviceSynchronize();
			//free cuda memory
			LOG_DEBUG << "executeFunction: 5: About to free CUDA kernel args";
			free(args);

			LOG_DEBUG << "executeFunction : 6";
			mUnusedContexts.write(contextToUse);
			}
		catch(...)
			{
			LOG_DEBUG << "executeFunction : 7";
			mUnusedContexts.write(contextToUse);
			throw;
			}
		}
private:
	int mDeviceCount;

	Queue<CUcontext> mUnusedContexts;

	std::vector<CUdevice> mCuDevices;

	std::vector<CUcontext> mCuContexts;

	boost::mutex mMutex;

	map<pair<CUcontext, string>, CUfunction> mCuFunctionsByName;
};

CUDAExecutionContext::CUDAExecutionContext() :
		mCUDAState(new CUDAExecutionContextInternalState())
	{
	}

void	CUDAExecutionContext::define(
						const std::string& inKernelName,
						const NativeCFG& inCFG,
						const Type& inInputType,
						const std::vector<Type>& inOutputTypes
						)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	// TODO:: assert that inCFG.returnTypes() match inOutputTypes
	string 	ptxDefinition =
		computePTXVectorApplyKernelFromNativeCFG(inCFG, inKernelName);

	// std::ofstream outF("generatedPtx.ptx");
	// outF << ptxDefinition;
	// outF.close();

	mCUDAState->addCudaModule(inKernelName, inKernelName, ptxDefinition);

	mNativeKernelsByName[inKernelName] = inCFG;
	mPTXKernelFunctionNames[inKernelName] = inKernelName;
	mPTXKernelsByName[inKernelName] = ptxDefinition;

	NativeType inputNativeType = cudaNativeTypeFor(inInputType);

	std::vector<NativeType> outputNativeTypes;
	outputNativeTypes.resize(inOutputTypes.size());
	for (long k = 0; k < outputNativeTypes.size(); ++k)
		{
		outputNativeTypes[k] = cudaNativeTypeFor(inOutputTypes[k]);
		}
	auto res = mInputOutputTypesByName.insert(
		make_pair(
			inKernelName,
			CUDAExecutionContext::InputOutputTypes(inInputType, inOutputTypes, inputNativeType, outputNativeTypes)
			)
		);
	lassert(res.second);
	}

ImplValContainer    createFORAVector(
						uint8_t* indexData,
						uword_t  indexAlignedSize, // we can also pass the Type if necessary
						const std::vector<Type>& outputElementTypes,
						const std::vector<NativeType>& outputElementNativeTypes,
						const std::vector<uint8_t*> alignedDataVectors,
						uint64_t count,
						MemoryPool* inPool
						)
	{
	if (!count || outputElementTypes.size() == 0 || alignedDataVectors.size() == 0)
		{
		LOG_WARN << "GPU returning empty vector because we have no elements.";
		return ImplValContainerUtilities::createVector(VectorRecord());
		}

	lassert(inPool);

	LOG_DEBUG << "createFORAVector: 1: Starting";
	TypedFora::Abi::ForaValueArray* array =
		TypedFora::Abi::ForaValueArray::Empty(inPool);

	std::set<JOV> jovSet;
	std::vector<uint8_t*> slots(count);
	std::vector<JOV> jovs(count);
	LOG_DEBUG << "createFORAVector: 2";
	if (indexData)
		{
		LOG_DEBUG << "createFORAVector: 3.0";
		for (uword_t k = 0; k < count; ++k)
			{
			lassert(indexData);
			LOG_DEBUG << "createFORAVector: 3.0.1 (k=" << k << ")." << (uint64_t)indexData;
			uint8_t index = indexData[k];
			LOG_DEBUG << "createFORAVector: 3.0.2 (k=" << k << "). type-index = " << (int)index;
			for (uword_t i = 0; i < outputElementTypes.size(); ++i)
				{
				LOG_DEBUG << "[lower byte]outputVal_" << i << "[" << k << "]= "
					<< (long)alignedDataVectors[i][k*outputElementNativeTypes[i].alignedSize()]
					<< ", type: " << prettyPrintString(outputElementNativeTypes[i]);
				}
			if (index == GPU_EXCEPTION_INDEX)
				{
				LOG_WARN << "GPU returning empty vector because we had an exception.";
				return ImplValContainerUtilities::createVector(VectorRecord());
				}

			lassert_dump(index < outputElementTypes.size(),
					"index = " << (long)index << ", elmtTypes = " << outputElementTypes.size());
			const Type& t = outputElementTypes[index];
			const NativeType& nt = outputElementNativeTypes[index];
			slots[k] = alignedDataVectors[index] + (k * nt.alignedSize());
			jovs[k] = JOV::OfType(t);
			jovSet.insert(jovs[k]);
			}
		}
	else {
		LOG_DEBUG << "createFORAVector: 3.1";
		jovSet.insert(JOV::OfType(outputElementTypes[0]));
		}

	LOG_DEBUG << "createFORAVector: 4 ";
	lassert(jovSet.size() > 0);
	if (jovSet.size() == 1)
		{
		const JOV& objectType = *jovSet.begin();
		lassert(objectType.type());
		const Type t = *objectType.type();
		const NativeType nativeTyp = cudaNativeTypeFor(t);

		uint8_t* alignedData;
		if (indexData)
			alignedData = alignedDataVectors[indexData[0]];
		else
			alignedData = alignedDataVectors[0];

		TypedFora::Abi::PackedForaValues vals = array->appendUninitialized(objectType, count);
		if (nativeTyp.alignedSize() != nativeTyp.packedSize())
			{
			uint8_t* packedData  = vals.data();
			for (int i = 0; i < count; ++i) {
				copyAlignedToPacked(nativeTyp, alignedData, packedData);
				alignedData += nativeTyp.alignedSize();
				packedData += nativeTyp.packedSize();
				}
			}
		else {
			lassert(objectType.type()->size() == nativeTyp.packedSize());
			memcpy(vals.data(), alignedData, objectType.type()->size() * count);
			}
		}
	else
		{
		TypedFora::Abi::slottedAppend(slots, jovs, jovSet, array);
		}

	VectorRecord vector(
		inPool->construct<VectorHandle>(
			Fora::BigVectorId(),
			Fora::PageletTreePtr(),
			array,
			inPool,
			ExecutionContext::currentExecutionContext()->newVectorHash()
			)
		);
	return ImplValContainerUtilities::createVector(vector);
	}


ImplValContainer	CUDAExecutionContext::executeKernel(
						const std::string&	inKernelName,
						ImplValContainer	inApplyObject,
						ImplValContainer	inSourceVector
						)
	{
	Type closureType = inApplyObject.type();
	Type inputElementType;
	NativeType inputElementNativeType;
	const std::vector<Type>* outputElementTypes;
	const std::vector<NativeType>* outputElementNativeTypes;
	bool mayThrowException = false;

		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		//verify that this kernel exists
		auto inoutIt = mInputOutputTypesByName.find(inKernelName);
		lassert_dump(
				inoutIt != mInputOutputTypesByName.end(),
				"used kernel " << inKernelName << " without defining it."
				);
		lassert_dump(
				mMayThrowException.find(inKernelName) != mMayThrowException.end(),
				"used kernel " << inKernelName << " without defining mMayThrowException for it."
				);

		// inputElementType = mInputOutputTypesByName[inKernelName].first;
		// outputElementTypes = &mInputOutputTypesByName[inKernelName].second;
		auto& inout = inoutIt->second;
		inputElementType = inout.inputType;
		outputElementTypes = &inout.outputTypes;
		lassert(outputElementTypes);

		inputElementNativeType = inout.inputNativeType;
		outputElementNativeTypes = &inout.outputNativeTypes;
		lassert(outputElementNativeTypes);

		mayThrowException = mMayThrowException[inKernelName];
		}

	lassert(inApplyObject.type().isClass());
	lassert(inSourceVector.type().isVector());

	TypedFora::Abi::VectorRecord sourceVectorHandle = inSourceVector.cast<TypedFora::Abi::VectorRecord>();

	lassert(sourceVectorHandle.size() &&
			sourceVectorHandle.jor().size() == 1 &&
			sourceVectorHandle.jor()[0].type());

	lassert_dump(
		*sourceVectorHandle.jor()[0].type() == inputElementType,
		"passed kernel " << inKernelName << " vector with elements of type "
				<< prettyPrintString(*sourceVectorHandle.jor()[0].type())
				<< " but expected" << prettyPrintString(inputElementType)
		);

	//bail if there are no elements
	if (!sourceVectorHandle.size())
		return ImplValContainer(
			CSTValue::blankOf(
				Type::Vector()
				)
			);

	uword_t elementCount = sourceVectorHandle.size();

	//create a aligned vectors
	AlignmentManager alignMgr;
	uint8_t* cudaClosureData = alignMgr.getHandleToCudaAlignedData(inApplyObject);
	uint8_t* cudaInVecData   = alignMgr.getHandleToCudaAlignedData(inSourceVector);

	if (!cudaInVecData || (!cudaClosureData && inApplyObject.type().size() > 0))
		return ImplValContainer(CSTValue::blankOf(Type::Vector()));

	uword_t outputElementTypesCount = outputElementTypes->size();
	lassert_dump(outputElementTypesCount < GPU_EXCEPTION_INDEX, // reserve value 0xFF to flag exceptions
			"Too many output types for 8bit unsigned integer to encode");
	// Type indexElementType = Type::Integer(8, false);
	NativeType indexElementType = NativeType::uint8();

	uint8_t* cudaOutVecIndex = nullptr;
	if (outputElementTypesCount > 1 || mayThrowException) {
		cudaOutVecIndex = alignMgr.allocateCudaAlignedData(indexElementType, elementCount);
		lassert(cudaOutVecIndex != nullptr);
		LOG_DEBUG << "allocated " << elementCount << " slots in the cuda result index vector (using CUDA unified memory)";
		memset(cudaOutVecIndex, 0xff, elementCount * indexElementType.alignedSize());
		LOG_DEBUG << "executeKernel(2): initialized CUDA outVecIdx with " << (int)cudaOutVecIndex[0]
				<< ", at address " << (uint64_t)cudaOutVecIndex;
	}
	LOG_DEBUG << "executeKernel(2): 4";

	std::vector<uint8_t*> cudaOutVecDataVectors(outputElementTypesCount);

	for (uword_t k = 0; k < outputElementTypesCount; ++k)
		{
		// const Type& t = (*outputElementTypes)[k];
		const NativeType& nt = (*outputElementNativeTypes)[k];

		uint8_t* ptr = alignMgr.allocateCudaAlignedData(nt, elementCount);
		lassert(ptr != nullptr);
		memset(ptr, k+1, elementCount * nt.alignedSize()); // For debugging purposes
		if (!ptr)
			return ImplValContainer(CSTValue::blankOf(Type::Vector()));
		cudaOutVecDataVectors[k] = ptr;
		}

	lassert(sourceVectorHandle.dataPtr()->unpagedValues());
	lassert(sourceVectorHandle.dataPtr()->unpagedValues()->size() == elementCount);

	mCUDAState->executeFunction(
		inKernelName,
		cudaClosureData,
		elementCount,
		cudaInVecData,
		cudaOutVecDataVectors,
		cudaOutVecIndex
		);

	lassert(ExecutionContext::currentExecutionContext());
	MemoryPool* pool = ExecutionContext::currentExecutionContext()->getMemoryPool();

	LOG_DEBUG << "executeKernel(2): 5: about to createFORAVector";

	auto res = createFORAVector(
					cudaOutVecIndex,
					indexElementType.alignedSize(),
					*outputElementTypes,
					*outputElementNativeTypes,
					cudaOutVecDataVectors,
					elementCount,
					pool);

	LOG_DEBUG << "executeKernel(2): 8: DONE";
	return res;
	}


ImplValContainer	CUDAExecutionContext::executeKernel(
						ImplValContainer	inApplyObject,
						ImplValContainer	inSourceVector
						)
	{
	Type vecElementType = *inSourceVector.cast<VectorRecord>().jor()[0].type();
	LOG_DEBUG << "CUDAApply Vector Element Type:" << prettyPrintString(vecElementType);
	LOG_DEBUG << "CUDAApply Function Object:" << prettyPrintString(inApplyObject);

	if (!vecElementType.isPOD())
		{
		throw UnableToConvertToPTX(
			prettyPrintString(vecElementType) + " isn't POD");
		}

	JudgmentOnValue	funJOV =
		JudgmentOnValue::Constant(inApplyObject.getReference()).relaxedJOV();

	string kernelName = "CUDA_" + hashToString(funJOV.hash() + vecElementType.hash());

		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		if (mNativeKernelsByName.find(kernelName) == mNativeKernelsByName.end())
			{
			ImmutableTreeVector<JudgmentOnValue> signatureJOVs =
				emptyTreeVec() +
					funJOV +
					JudgmentOnValue::Constant(CSTValue(Symbol::Call())) +
					JudgmentOnValue::OfType(vecElementType)
				;
			LOG_DEBUG << "signature JOVs: " << prettyPrintString(signatureJOVs);

			PolymorphicSharedPtr<Fora::SimpleForwardReasoner> reasoner(
				new Fora::SimpleForwardReasoner(
					Runtime::getRuntime().getTypedForaCompiler(),
		            Runtime::getRuntime().getInstructionGraph(),
		            Runtime::getRuntime().getAxioms()
		            )
				);

			auto frame = reasoner->reasonAboutApply(JOVT::Unnamed(signatureJOVs));

			if (frame->exits().resultPart().size() < 1 )
				{
				ostringstream msg;
				msg << "Code returns zero types ("
						<< frame->exits().resultPart().size() << "):"
						<< prettyPrintString(frame->exits().resultPart());
				throw UnableToConvertToPTX(msg.str());
				}

			auto res = reasoner->compileEntrypointForApply(JOVT::Unnamed(signatureJOVs));

			if (!res)
				throw UnableToConvertToPTX(
					"Reasoning failed to converge."
					);

			//the target instruction might have different JOVS
			//than the signature, because the axiom might be
			//a 'class apply' in which case the instruction
			//will have all of the class object's members
			//as arguments as well. These will all be constants
			//in this case, and will be removed
			//from the NativeCFG because they're all passed as
			//'none'
			ImmutableTreeVector<JudgmentOnValue>
				targetInstructionJOVs = signatureJOVs;

			//if the joa() has multiple exit points that are compatible,
			//we need to wrap them up into a single typed value
			bool	needsReturnTypeModification = false;
			TypedFora::Converter converter;

			NativeCFG cfg =
					converter.convertCallable(
							Runtime::getRuntime().getTypedForaCompiler()->getDefinition(res->second));

			lassert(res->first == TypedFora::BlockID::entry());

			// LOG_DEBUG << "CFG before any processing: "
					// << prettyPrintString(cfg);

			cfg = NativeCFGTransforms::convertForGpuExecution(cfg, frame->exits().resultPart().size());
			LOG_DEBUG << "CFG after initial processing";

			while (cfg.externalBranches().size())
				{
				string nameOfSubbranch = cfg.externalBranches()[0];
				if (Runtime::getRuntime().getTypedForaCompiler()->
						getMutuallyRecursiveFunctions(nameOfSubbranch).size())
					{
					throw UnableToConvertToPTX("contains recursion");
					}
				NativeCFG cfgToInline =
						converter.convertCallable(
								Runtime::getRuntime().getTypedForaCompiler()
									->getDefinition(nameOfSubbranch));
				cfg = NativeCFGTransforms::inlineCFG(cfg,
					NativeCFGTransforms::convertForGpuExecution(
						cfgToInline,
						cfgToInline.returnTypes().size()
						),
					nameOfSubbranch
					);
				LOG_DEBUG << "CFG after processing inlined code";
				}

			try {
				mMayThrowException[kernelName] = true;
				//a lot simpler when all variables are unique
				cfg = NativeCFGTransforms::optimize(cfg, Runtime::getRuntime().getConfig());
				cfg = NativeCFGTransforms::renameVariables(cfg);

				std::vector<Type> outputTypes(frame->exits().resultPart().size());
					{
					long k = 0;
					for (auto& jov : frame->exits().resultPart().vals())
						{
						Nullable<Type> t = jov.type();
						lassert_dump(
								t,
								"return type at index " << k << " of CUDA kernel '"
									<< kernelName << "' is missing");
						outputTypes[k] = *t;
						LOG_DEBUG << " output_type[" << k << "] = " << prettyPrintString(*t);
						++k;
						}
					}
				LOG_DEBUG << "executeKernel: 7";
				
				LOG_TEST << "code pre-define " << cfg;

				define(kernelName, cfg, vecElementType, outputTypes);
				}
			catch(std::logic_error e)
				{
				throw UnableToConvertToPTX("internal error: " +
					string(e.what()));
				}
			}
		}
	return executeKernel(kernelName, inApplyObject, inSourceVector);
	}
