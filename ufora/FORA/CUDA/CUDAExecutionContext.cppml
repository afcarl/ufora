/***************************************************************************
    Copyright 2015 Ufora Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/

#include "CUDAExecutionContext.hppml"
#include "CUDAVectorRecord.hpp"
#include "NativeCFGToPTX.hppml"
#include "Alignment.hpp"
#include "AlignmentManager.hpp"
#include "../Core/ExecutionContext.hppml"
#include "../Core/MemoryPool.hpp"
#include "../Core/ImplValContainerUtilities.hppml"
#include "../Native/NativeCode.hppml"
#include "../Native/NativeType.hppml"
#include "../Native/NativeTypeFor.hpp"
#include "../TypedFora/JitCompiler/StaticInliner.hppml"
#include "../TypedFora/Converter.hppml"
#include "../TypedFora/ABI/NativeLayoutType.hppml"
#include "../TypedFora/ABI/SlottedForaValueArrayAppend.hppml"
#include "../TypedFora/ABI/VectorRecord.hpp"
#include "../TypedFora/ABI/VectorRecordCodegen.hppml"
#include "../TypedFora/ABI/VectorHandle.hpp"
#include "../TypedFora/TypedFora.hppml"
#include "../TypedFora/TypedForaUtil.hppml"
#include "../VectorDataManager/PageletTree.hppml"
#include "GpuCodegen.hppml"
#include "../../core/Logging.hpp"
#include "../../core/threading/Queue.hpp"

#include <cuda.h>
#include <cuda_runtime_api.h>

#include <fstream>
#include <iostream>

using TypedFora::Abi::VectorHandle;
using TypedFora::Abi::VectorRecord;
using Fora::Interpreter::ExecutionContext;

constexpr uint8_t GPU_EXCEPTION_INDEX = 0xFF;

NativeType cudaNativeTypeFor(const Type& type)
	{
	static NativeType vec = NativeTypeFor<TypedFora::Abi::VectorRecord>::get();
	auto res = TypedFora::Abi::nativeLayoutType(type);
	if (res == vec)
		{
		return NativeTypeFor<CUDAVectorRecord>::get();
		}
	else
		{
		return res;
		}
	}

class CUDAExecutionContextInternalState {
public:

	CUDAExecutionContextInternalState() : mDeviceCount(0)
		{
		cudaDriverInitializer();
		cudaDeviceInitializer();
		}


	//initialize CUDA. single threaded, only happens once
	static	void	cudaDriverInitializer()
		{
		static boost::recursive_mutex	mMutex;

		static bool isInitialized = false;

		if (!isInitialized)
			{
			boost::recursive_mutex::scoped_lock	lock(mMutex);
			if (!isInitialized)
				{
				//initialize the driver
				CUresult error = cuInit(0);
				throwOnError(error, "Initialize cuda");
				isInitialized = true;
				}
			}
		}

	static void throwOnError(CUresult error, std::string op)
		{
		if (error != CUDA_SUCCESS)
			{
			const char* ptr = 0;
			cuGetErrorString(error, &ptr);
			throw UnableToConvertToPTX("Couldn't " + op + ": " + std::string(ptr ? ptr : "") +
				". code=" + boost::lexical_cast<string>(error));
			}
		}

	void	cudaDeviceInitializer()
		{
		CUresult error = cuDeviceGetCount(&mDeviceCount);
		throwOnError(error, "Get a device count");

		lassert_dump(mDeviceCount, "there are no CUDA-enabled devices");

		//take the first device
		mCuDevices.resize(mDeviceCount);
		mCuContexts.resize(mDeviceCount);

		for (long devID = 0; devID < mDeviceCount; devID++)
			{
			int major, minor;
			constexpr int DEVICE_NAME_MAX_LENGTH = 256;
			char deviceName[DEVICE_NAME_MAX_LENGTH];

			throwOnError(
				cuDeviceComputeCapability(&major, &minor, devID),
				"get CUDA device compute capability"
				);
			throwOnError(
				cuDeviceGetName(deviceName, DEVICE_NAME_MAX_LENGTH, devID),
				"get CUDA device name"
				);

			LOG_INFO 	<< "CUDA Using Device " << devID
						<< ": \"" << deviceName
						<< "\" with Compute " << major << "." << minor
						<< " capability.\n"
						;
			if (major < 3) {
				// We need compute capability to be at least 3.0 to use CUDA unified memory
				throw UnableToConvertToPTX("GPU Compute Capability needed: >=3.0. "
						+ boost::lexical_cast<string>("Incompatible GPU Compute Capability detected: ")
						+ boost::lexical_cast<string>(major) + "." + boost::lexical_cast<string>(minor));
				}
			// pick up device with zero ordinal (default, or devID)
			throwOnError(
				cuDeviceGet(&mCuDevices[devID], devID),
				"get CUDA device handle"
				);

			// Create context
			throwOnError(
				cuCtxCreate(&mCuContexts[devID], 0, mCuDevices[devID]),
				"create CUDA execution context for device"
				);
			}

		for (auto c: mCuContexts)
			mUnusedContexts.write(c);
		}

	string addLineNumbers(const string& str)
		{
		ostringstream outStr;

		uint32_t lastIx = 0;
		uint32_t lineNo = 1;

		for (long k = 0; k < str.size();k++)
			if (str[k] == '\n' || k == str.size() - 1)
				{
				outStr << lineNo << "   ";
				lineNo++;

				outStr << str.substr(lastIx, k - lastIx) << "\n";
				lastIx = k+1;
				}

		return outStr.str();
		}

	void addCudaModule(	const string& inModuleName,
						const string& inFunctionName,
						const string& inPTXCode)
		{
		for (auto context: mCuContexts)
			{
			throwOnError(
				cuCtxSetCurrent(context),
				"set CUDA execution context"
				);

			const unsigned int jitNumOptions = 5;
			CUjit_option *jitOptions = new CUjit_option[jitNumOptions];
			void **jitOptVals = new void*[jitNumOptions];

			int bufferSize = 1024;
			char	jitLogBuffer[1024];
			char	jitErrorBuffer[1024];

			jitOptions[0] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
			jitOptVals[0] = (void *)(size_t)bufferSize;
			jitOptions[1] = CU_JIT_INFO_LOG_BUFFER;
			jitOptVals[1] = jitLogBuffer;

			jitOptions[2] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
			jitOptVals[2] = (void*)(size_t)bufferSize;
			jitOptions[3] = CU_JIT_ERROR_LOG_BUFFER;
			jitOptVals[3] = jitErrorBuffer;

			jitOptions[4] = CU_JIT_MAX_REGISTERS;
			int jitRegCount = 32;
			jitOptVals[4] = (void *)(size_t)jitRegCount;

			CUmodule	cuModule;

			CUresult error = cuModuleLoadDataEx(&cuModule, inPTXCode.c_str(),
							jitNumOptions, jitOptions, (void **)jitOptVals);

			if (error != CUDA_SUCCESS)
				{
				LOG_WARN << "CUDA FAILED TO COMPILE: " +
												string(jitLogBuffer) + "\n" +
												string(jitErrorBuffer) + "\n" +
												"\n\nPTX code = \n\n" +
												addLineNumbers(inPTXCode);

				lassert_dump(false, "CUDA FAILED TO COMPILE: " +
												string(jitLogBuffer) + "\n" +
												string(jitErrorBuffer) + "\n" +
												"\n\nPTX code = \n\n" +
												addLineNumbers(inPTXCode)
												);
				}

			CUfunction cuFun;

			// Get function handle from module
			throwOnError(
				cuModuleGetFunction(&cuFun, cuModule, inFunctionName.c_str()),
				std::string("get function '") + inFunctionName + "' in CUDA Module"
				);

				{
				boost::mutex::scoped_lock lock(mMutex);
				mCuFunctionsByName[make_pair(context, inModuleName)] = cuFun;
				}
			}

		}


	CUfunction functionNameForModule(CUcontext context, std::string inModuleName)
		{
		boost::mutex::scoped_lock lock(mMutex);
		return mCuFunctionsByName[make_pair(context, inModuleName)];
		}

	void executeFunction(	const std::string&				inModuleName,
							uint8_t*						inCudaClosureData,
							uword_t							inN,
							uint8_t*						inCudaSourceData,
							std::vector<uint8_t*>&			outCudaDestDataVectors,
							uint8_t*						outCudaIndexData
							)
		{
		lassert(outCudaDestDataVectors.size() > 0);

		uword_t outputTypesCount = outCudaDestDataVectors.size();
		LOG_DEBUG << "executeFunction : 1: outputTypesCount = " << outputTypesCount;

		CUcontext contextToUse = mUnusedContexts.get();

		try {
			throwOnError(
				cuCtxSetCurrent(contextToUse),
				"set CUDA execution context"
				);
			std::vector<CUdeviceptr> d_destData(outputTypesCount);

			CUresult error;

			// Grid/Block configuration
			int threadsPerBlock = 1024;
			int blocksPerGrid   = (inN + threadsPerBlock - 1) / threadsPerBlock;

			std::vector<void*> args;

			if (inCudaClosureData)
				args.push_back(&inCudaClosureData);

			args.push_back(&inN);
			args.push_back(&inCudaSourceData);

			if (outCudaIndexData)
				args.push_back(&outCudaIndexData);

			for (uword_t k = 0; k < outputTypesCount; ++k)
				args.push_back(&outCudaDestDataVectors[k]);

			// Launch the CUDA kernel
			LOG_DEBUG << "executeFunction: 4: About to launch CUDA kernel";

			error = cuLaunchKernel( functionNameForModule(contextToUse, inModuleName),
									blocksPerGrid, 1, 1,
									threadsPerBlock, 1, 1,
									0,
									NULL, (void**)&args[0], NULL);
			throwOnError(error, "Launch Kernel: ");
			lassert_dump(error == CUDA_SUCCESS, "UNKNOWN CUDA ERROR");
			cudaDeviceSynchronize();

			mUnusedContexts.write(contextToUse);
			}
		catch(...)
			{
			LOG_DEBUG << "executeFunction : 7";
			mUnusedContexts.write(contextToUse);
			throw;
			}
		}
	
private:
	int mDeviceCount;

	Queue<CUcontext> mUnusedContexts;

	std::vector<CUdevice> mCuDevices;

	std::vector<CUcontext> mCuContexts;

	boost::mutex mMutex;

	map<pair<CUcontext, string>, CUfunction> mCuFunctionsByName;
};

CUDAExecutionContext::CUDAExecutionContext() :
		mCUDAState(new CUDAExecutionContextInternalState())
	{
	}

void	CUDAExecutionContext::define(
						const std::string& inKernelName,
						const NativeCFG& inCFG,
						const Type& inInputType,
						const ImmutableTreeVector<Type>& inOutputTypes
						)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	// TODO:: assert that inCFG.returnTypes() match inOutputTypes
	string 	ptxDefinition =
		computePTXVectorApplyKernelFromNativeCFG(inCFG, inKernelName);

	// std::ofstream outF("generatedPtx.ptx");
	// outF << ptxDefinition;
	// outF.close();

	mCUDAState->addCudaModule(inKernelName, inKernelName, ptxDefinition);

	mNativeKernelsByName[inKernelName] = inCFG;
	mPTXKernelFunctionNames[inKernelName] = inKernelName;
	mPTXKernelsByName[inKernelName] = ptxDefinition;

	NativeType inputNativeType = cudaNativeTypeFor(inInputType);

	std::vector<NativeType> outputNativeTypes;
	outputNativeTypes.resize(inOutputTypes.size());
	for (long k = 0; k < outputNativeTypes.size(); ++k)
		{
		outputNativeTypes[k] = cudaNativeTypeFor(inOutputTypes[k]);
		}
	auto res = mInputOutputTypesByName.insert(
		make_pair(
			inKernelName,
			CUDAExecutionContext::InputOutputTypes(inInputType, inOutputTypes, inputNativeType, outputNativeTypes)
			)
		);
	lassert(res.second);
	}

ImplValContainer    createFORAVector(
						uint8_t* indexData,
						uword_t  indexAlignedSize, // we can also pass the Type if necessary
						const ImmutableTreeVector<Type>& outputElementTypes,
						const ImmutableTreeVector<NativeType>& outputElementNativeTypes,
						const std::vector<uint8_t*>& alignedDataVectors,
						uint64_t count,
						MemoryPool* inPool
						)
	{
	if (!count || outputElementTypes.size() == 0 || alignedDataVectors.size() == 0)
		{
		LOG_WARN << "GPU returning empty vector because we have no elements.";
		return ImplValContainerUtilities::createVector(VectorRecord());
		}

	lassert(inPool);

	LOG_DEBUG << "createFORAVector: 1: Starting";
	TypedFora::Abi::ForaValueArray* array =
		TypedFora::Abi::ForaValueArray::Empty(inPool);

	std::set<JOV> jovSet;
	std::vector<uint8_t*> slots(count);
	std::vector<JOV> jovs(count);
	std::vector<JOV> outTypesAsJovs;
	for (auto t: outputElementTypes)
		outTypesAsJovs.push_back(JOV::OfType(t));


	LOG_DEBUG << "createFORAVector: 2";
	if (indexData)
		{
		std::vector<bool> jovMask;
		jovMask.resize(outputElementTypes.size());
		LOG_DEBUG << "createFORAVector: 3.0";
		for (uword_t k = 0; k < count; ++k)
			{
			lassert(indexData);
			LOG_DEBUG << "createFORAVector: 3.0.1 (k=" << k << ")." << (uint64_t)indexData;
			uint8_t index = indexData[k];
			LOG_DEBUG << "createFORAVector: 3.0.2 (k=" << k << "). type-index = " << (int)index;
			for (uword_t i = 0; i < outputElementTypes.size(); ++i)
				{
				LOG_DEBUG << "[lower byte]outputVal_" << i << "[" << k << "]= "
					<< (long)alignedDataVectors[i][k*outputElementNativeTypes[i].alignedSize()]
					<< ", type: " << prettyPrintString(outputElementNativeTypes[i]);
				}
			if (index == GPU_EXCEPTION_INDEX)
				{
				LOG_WARN << "GPU returning empty vector because we had an exception.";
				return ImplValContainerUtilities::createVector(VectorRecord());
				}

			lassert_dump(index < outputElementTypes.size(),
					"index = " << (long)index << ", elmtTypes = " << outputElementTypes.size());
			const Type& t = outputElementTypes[index];
			const NativeType& nt = outputElementNativeTypes[index];
			slots[k] = alignedDataVectors[index] + (k * nt.alignedSize());
			jovs[k] = outTypesAsJovs[index];
			jovMask[index] = true;
			}

		for (long index = 0; index < jovMask.size(); index++)
			if (jovMask[index])
				jovSet.insert(outTypesAsJovs[index]);
		}
	else {
		LOG_DEBUG << "createFORAVector: 3.1";
		jovSet.insert(JOV::OfType(outputElementTypes[0]));
		}

	LOG_DEBUG << "createFORAVector: 4 ";
	lassert(jovSet.size() > 0);
	if (jovSet.size() == 1)
		{
		const JOV& objectType = *jovSet.begin();
		lassert(objectType.type());
		const Type t = *objectType.type();
		const NativeType nativeTyp = cudaNativeTypeFor(t);

		uint8_t* alignedData;
		if (indexData)
			alignedData = alignedDataVectors[indexData[0]];
		else
			alignedData = alignedDataVectors[0];

		TypedFora::Abi::PackedForaValues vals = array->appendUninitialized(objectType, count);
		if (nativeTyp.alignedSize() != nativeTyp.packedSize())
			{
			uint8_t* packedData  = vals.data();
			for (int i = 0; i < count; ++i) {
				copyAlignedToPacked(nativeTyp, alignedData, packedData);
				alignedData += nativeTyp.alignedSize();
				packedData += nativeTyp.packedSize();
				}
			}
		else {
			lassert(objectType.type()->size() == nativeTyp.packedSize());
			memcpy(vals.data(), alignedData, objectType.type()->size() * count);
			}
		}
	else
		{
		TypedFora::Abi::slottedAppend(slots, jovs, jovSet, array);
		}

	VectorRecord vector(
		inPool->construct<VectorHandle>(
			Fora::BigVectorId(),
			Fora::PageletTreePtr(),
			array,
			inPool,
			ExecutionContext::currentExecutionContext()->newVectorHash()
			)
		);
	return ImplValContainerUtilities::createVector(vector);
	}


ImplValContainer	CUDAExecutionContext::executeKernel(
						const std::string&	inKernelName,
						ImplValContainer	inApplyObject,
						ImplValContainer	inSourceVector
						)
	{
	Type closureType = inApplyObject.type();
	Type inputElementType;
	NativeType inputElementNativeType;
	ImmutableTreeVector<Type> outputElementTypes;
	ImmutableTreeVector<NativeType> outputElementNativeTypes;
	
		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		//verify that this kernel exists
		auto inoutIt = mInputOutputTypesByName.find(inKernelName);
		lassert_dump(
				inoutIt != mInputOutputTypesByName.end(),
				"used kernel " << inKernelName << " without defining it."
				);
		
		// inputElementType = mInputOutputTypesByName[inKernelName].first;
		// outputElementTypes = &mInputOutputTypesByName[inKernelName].second;
		auto& inout = inoutIt->second;
		inputElementType = inout.inputType;
		outputElementTypes = inout.outputTypes;
		
		inputElementNativeType = inout.inputNativeType;
		outputElementNativeTypes = inout.outputNativeTypes;
		}

	lassert(inApplyObject.type().isClass());
	lassert(inSourceVector.type().isVector());

	TypedFora::Abi::VectorRecord sourceVectorHandle = inSourceVector.cast<TypedFora::Abi::VectorRecord>();

	lassert(sourceVectorHandle.size() &&
			sourceVectorHandle.jor().size() == 1 &&
			sourceVectorHandle.jor()[0].type());

	lassert_dump(
		*sourceVectorHandle.jor()[0].type() == inputElementType,
		"passed kernel " << inKernelName << " vector with elements of type "
				<< prettyPrintString(*sourceVectorHandle.jor()[0].type())
				<< " but expected" << prettyPrintString(inputElementType)
		);

	//bail if there are no elements
	if (!sourceVectorHandle.size())
		return ImplValContainer(
			CSTValue::blankOf(
				Type::Vector()
				)
			);

	uword_t elementCount = sourceVectorHandle.size();

	//create a aligned vectors
	AlignmentManager alignMgr;
	uint8_t* cudaClosureData = alignMgr.getHandleToCudaAlignedData(inApplyObject);
	CudaVectorRecordStorage* cudaInVecData = (CudaVectorRecordStorage*)alignMgr.getHandleToCudaAlignedData(inSourceVector);

	if (!cudaInVecData || (!cudaClosureData && inApplyObject.type().size() > 0))
		return ImplValContainer(CSTValue::blankOf(Type::Vector()));

	uword_t outputElementTypesCount = outputElementTypes.size();
	lassert_dump(outputElementTypesCount < GPU_EXCEPTION_INDEX, // reserve value 0xFF to flag exceptions
			"Too many output types for 8bit unsigned integer to encode");
	// Type indexElementType = Type::Integer(8, false);
	NativeType indexElementType = NativeType::uint8();

	uint8_t* cudaOutVecIndex = nullptr;

	cudaOutVecIndex = alignMgr.allocateCudaMemory(indexElementType.alignedSize() * elementCount);
	lassert(cudaOutVecIndex != nullptr);
	LOG_DEBUG << "allocated " << elementCount << " slots in the cuda result index vector (using CUDA unified memory)";
	memset(cudaOutVecIndex, 0xff, elementCount * indexElementType.alignedSize());
	LOG_DEBUG << "executeKernel(2): initialized CUDA outVecIdx with " << (int)cudaOutVecIndex[0]
			<< ", at address " << (uint64_t)cudaOutVecIndex;

	std::vector<uint8_t*> cudaOutVecDataVectors(outputElementTypesCount);

	for (uword_t k = 0; k < outputElementTypesCount; ++k)
		{
		// const Type& t = (*outputElementTypes)[k];
		const NativeType& nt = outputElementNativeTypes[k];

		uint8_t* ptr = alignMgr.allocateCudaMemory(nt.alignedSize() * elementCount);
		lassert(ptr != nullptr);
		memset(ptr, k+1, elementCount * nt.alignedSize()); // For debugging purposes
		if (!ptr)
			return ImplValContainer(CSTValue::blankOf(Type::Vector()));
		cudaOutVecDataVectors[k] = ptr;
		}

	mCUDAState->executeFunction(
		inKernelName,
		cudaClosureData,
		elementCount,
		cudaInVecData->mDataPtr,
		cudaOutVecDataVectors,
		cudaOutVecIndex
		);

	lassert(ExecutionContext::currentExecutionContext());
	MemoryPool* pool = ExecutionContext::currentExecutionContext()->getMemoryPool();

	LOG_DEBUG << "executeKernel(2): 5: about to createFORAVector";

	auto res = createFORAVector(
					cudaOutVecIndex,
					indexElementType.alignedSize(),
					outputElementTypes,
					outputElementNativeTypes,
					cudaOutVecDataVectors,
					elementCount,
					pool);

	LOG_DEBUG << "executeKernel(2): 8: DONE";
	return res;
	}


ImplValContainer	CUDAExecutionContext::executeKernel(
						ImplValContainer	inApplyObject,
						ImplValContainer	inSourceVector
						)
	{
	lassert_dump(
		inSourceVector.cast<VectorRecord>().jor().size() == 1,
		"expected input vector with one type but it has " << inSourceVector.cast<VectorRecord>().jor().size()
		);
	
	JOV vecElementJov = inSourceVector.cast<VectorRecord>().jor()[0];

	LOG_DEBUG << "CUDAApply Vector Element Type:" << prettyPrintString(vecElementJov);
	LOG_DEBUG << "CUDAApply Function Object:" << prettyPrintString(inApplyObject);

	JudgmentOnValue	funJOV = JudgmentOnValue::FromLiveValue(inApplyObject.getReference()).relaxedJOV();

	string kernelName = "CUDA_" + hashToString(funJOV.hash() + vecElementJov.hash());

		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		if (mNativeKernelsByName.find(kernelName) == mNativeKernelsByName.end())
			{
			try {
				pair<NativeCFG, ImmutableTreeVector<Type> > cfgAndOutputTypes = 
					computeGpuKernelFunction(funJOV, vecElementJov);

				define(kernelName, cfgAndOutputTypes.first, *vecElementJov.type(), cfgAndOutputTypes.second);
				}
			catch(std::logic_error e)
				{
				throw UnableToConvertToPTX("internal error: " +
					string(e.what()));
				}
			}
		}
	return executeKernel(kernelName, inApplyObject, inSourceVector);
	}
