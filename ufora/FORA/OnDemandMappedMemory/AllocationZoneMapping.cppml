/***************************************************************************
    Copyright 2016 Ufora Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/

#include "AllocationZoneMapping.hppml"
#include "MemoryViewDefinition.hppml"

AllocationZoneMapping::AllocationZoneMapping(int64_t pageSize) : mPageSize(pageSize)
    {
    }

void AllocationZoneMapping::discardMemoryView(const MemoryViewId& id)
    {
    lassert_dump(false, "not implemented");
    }

void AllocationZoneMapping::addMemoryView(const MemoryViewDefinition& layout)
    {
    map<int64_t, MemoryBlockByteRange>& slices(mMemoryViewBlockRangeMapping[layout.viewId()]);
    
    IntegerRanges<false>& ranges(mMemoryViewBlockBoundaries[layout.viewId()]);
    
    int64_t lastByteRange = 0;

    long cumulative = 0;
    for (auto term: layout.terms())
        {
        lassert_dump(term.isViewOfBlock(), "not");

        IntegerRange byterange =
            IntegerRange(cumulative, cumulative + term.bytecount());

        //we can map a portion of the Block zone into this one
        //round the low byterange up to pagesize. AllocationZone mappings are always
        //aligned on the page

        uint64_t blockBoundary = roundUpToPageSize(byterange.low());
        uint64_t blockBoundaryTop = roundDownToPageSize(byterange.high());

        if (blockBoundary < blockBoundaryTop)
            {
            if (blockBoundary > lastByteRange)
                {
                addMemoryViewZone(
                    layout.viewId(),
                    RangeToZoneMapping(
                        IntegerRange(lastByteRange, blockBoundary),
                        AllocationZone::View(layout.viewId()),
                        IntegerRange(lastByteRange, blockBoundary)
                        )
                    );
                }

            uint64_t firstValueOfMappingByteOffsetInBlock = term.byterange().low();

            uint64_t blockBoundaryByteOffsetInBlock =
                firstValueOfMappingByteOffsetInBlock + (blockBoundary - byterange.low());

            //we need to pick a byte offset for the first byte in the block so that
            //    blockBoundaryByteOffsetInBlock + blockByteOffset
            //is page aligned

            //this is the 'byte' that the first piece of block data will have to
            //reside in in the AllocationZone if we're going to use this block.
            uint64_t blockByteOffset = roundUpToPageSize(blockBoundaryByteOffsetInBlock) - blockBoundaryByteOffsetInBlock;

            lassert(blockByteOffset >= 0 && blockByteOffset < mPageSize);

            //this is the first byte of block data that 'blockBoundary' will represent
            uint64_t firstByteOfBlockDataReferencedByRange =
                blockByteOffset + blockBoundaryByteOffsetInBlock;

            lassert(firstByteOfBlockDataReferencedByRange % mPageSize == 0);

            addMemoryViewZone(
                layout.viewId(),
                RangeToZoneMapping(
                    IntegerRange(blockBoundary, blockBoundaryTop),
                    AllocationZone::Block(term.getViewOfBlock().block(), blockByteOffset), 
                    IntegerRange(
                        firstByteOfBlockDataReferencedByRange,
                        firstByteOfBlockDataReferencedByRange +
                            blockBoundaryTop - blockBoundary
                        )
                    )
                );

            lastByteRange = blockBoundaryTop;
            }
        ranges.addRange(byterange);

        slices[cumulative] = MemoryBlockByteRange(term.getViewOfBlock().block(), term.byterange());
        cumulative += term.bytecount();
        }

    if (cumulative > lastByteRange)
        {
        int64_t roundedUp = roundUpToPageSize(cumulative);

        addMemoryViewZone(
            layout.viewId(),
            RangeToZoneMapping(
                IntegerRange(lastByteRange, roundedUp),
                AllocationZone::View(layout.viewId()),
                IntegerRange(lastByteRange, roundedUp)
                )
            );
        }
    // FIXME: should we be updating cumulative after the preceding addMemoryViewZone
    mMemoryViewBytecountMapping[layout.viewId()] = cumulative;
    }

IntegerRange AllocationZoneMapping::zeroPaddingBytesFor(PhysicalMemoryAllocation alloc)
    {
    @match AllocationZone(alloc.zone())
        -| Block(blockId, byteOffset) ->> {
            return IntegerRange();
        }
        -| View(view) ->> {
            lassert(mMemoryViewBytecountMapping.count(view) != 0);
            int64_t bytecount = mMemoryViewBytecountMapping[view]; 

            IntegerRange byteRange = alloc.byteRange();

            if (alloc.byteRange().high() > bytecount)
                {
                byteRange.low() = bytecount; 
                return byteRange - alloc.byteRange().low();
                }
            else
                return IntegerRange();
        }

    }

RangeToZoneMapping AllocationZoneMapping::zoneMappingFor(const BlockOrViewId& object, IntegerRange byterange) const
    {
    lassert_dump(byterange == clipToPageSize(byterange), "range argument is intended to be a single page");

    @match BlockOrViewId(object)
        -| View(id) ->> {
            auto it = mMemoryViewZoneBoundaries.find(id);
            lassert(it != mMemoryViewZoneBoundaries.end());

            Nullable<IntegerRange> nRng = it->second.rangeContaining(byterange.low());

            lassert_dump(nRng, "can't find " << byterange.low() << " for memory view "
                << prettyPrintString(id) << " which has slices "
                << prettyPrintString(mMemoryViewBlockRangeMapping.find(id)->second)
                << " and block ranges " << prettyPrintString(it->second.getRangeStartsAndEnds())
                );


            IntegerRange range = *nRng;

            auto it2 = mMemoryViewZoneMappings.find(make_pair(id, range));

            lassert(it2 != mMemoryViewZoneMappings.end());

            return it2->second.restrictedToObjectRange(byterange);
            }
        -| Block(id) ->> {
            return RangeToZoneMapping(byterange, AllocationZone::Block(id, 0), byterange);
            }
    }

const std::map<AllocationZone, long>& AllocationZoneMapping::zonesActiveForBlock(const MemoryBlockId& block)
    {
    //ensure the base zone is populated
    mMemoryBlockZoneRefcounts[block][AllocationZone::Block(block, 0)];

    return mMemoryBlockZoneRefcounts[block];
    }

void AllocationZoneMapping::mapZoneRangeToBlockRanges(
            AllocationZone zone,
            IntegerRange byteRange,
            std::vector<ZoneToBlockMapping>& outRanges,
            IntegerRange& outBytesOfPaddingZeros
            )
    {
    outBytesOfPaddingZeros = IntegerRange();

    @match AllocationZone(zone)
        -| Block(id, byteOffset) ->> {
            outRanges.push_back(ZoneToBlockMapping(byteRange, id, byteRange - byteOffset));
            }
        -| View(view) ->> {
            std::vector<IntegerRange> ranges;

            mMemoryViewBlockBoundaries[view].rangesIntersecting(byteRange, ranges);

            for (auto range: ranges)
                {
                MemoryBlockByteRange blockRange = mMemoryViewBlockRangeMapping[view][range.low()];

                //this is the active range within the blockRange
                IntegerRange rangeWithinSlice = range.intersect(byteRange) - range.low();

                lassert(rangeWithinSlice.size() <= blockRange.byterange().size());

                IntegerRange valuesWithinBlock = rangeWithinSlice + blockRange.byterange().low();

                lassert(valuesWithinBlock.low() >= 0);

                outRanges.push_back(
                    ZoneToBlockMapping(
                        range.intersect(byteRange),
                        blockRange.blockId(),
                        valuesWithinBlock
                        )
                    );
                }

            lassert(mMemoryViewBytecountMapping.count(view) != 0);
            int64_t bytecount = mMemoryViewBytecountMapping[view]; 

            if (byteRange.high() > bytecount)
                outBytesOfPaddingZeros = IntegerRange(bytecount, byteRange.high());
            }

    int64_t totalBytes = outBytesOfPaddingZeros.size();

    for (auto mapping: outRanges)
        totalBytes += mapping.byteRangeInBlock().size();

    lassert(totalBytes == byteRange.size());
    }

uint64_t AllocationZoneMapping::roundUpToPageSize(uint64_t o) const
    {
    auto frac = o % mPageSize;
    if (frac)
        return o + mPageSize - frac;
    return o;
    }

uint64_t AllocationZoneMapping::roundDownToPageSize(uint64_t o) const
    {
    return o - o % mPageSize;
    }

IntegerRange AllocationZoneMapping::clipToPageSize(IntegerRange in) const
    {
    in.low() = roundUpToPageSize(in.low());
    in.high() = roundDownToPageSize(in.high());
    return in;
    }

int64_t AllocationZoneMapping::pageSize() const
    {
    return mPageSize;
    }

void AllocationZoneMapping::addMemoryViewZone(MemoryViewId id, RangeToZoneMapping mapping)
    {
    mMemoryViewZoneBoundaries[id].addRange(mapping.byteRangeInObject());
    mMemoryViewZoneMappings[make_pair(id, mapping.byteRangeInObject())] = mapping;

    @match AllocationZone(mapping.zoneContainingData())
        -| Block(bId) ->> {
            mMemoryBlockZoneRefcounts[bId][mapping.zoneContainingData()]++;
            }
        -| _ ->> {}
    }
