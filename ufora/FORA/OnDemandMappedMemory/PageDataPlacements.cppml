/***************************************************************************
    Copyright 2016 Ufora Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/

#include "PageDataPlacements.hppml"
#include "../../core/math/Largest.hpp"

PageDataPlacements::PageDataPlacements(
                        int64_t pageSize,
                        boost::function<uint8_t* (PhysicalMemoryAllocation alloc)> inAllocator,
                        boost::function<void (PhysicalMemoryAllocation alloc, uint8_t*)> inDeallocator,
                        boost::function<bool (uint8_t* sharedBase, uint64_t sharedOffset, uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount)> inMapper,
                        boost::function<bool (uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount)> inUnmapper,
                        boost::function<void (MemoryBlockId block, IntegerRange bytecountRange)> inBroadcaster,
                        int64_t maxExcessBytes,
                        int64_t maxMemoryViewAllocations
                        ) :
                    mBroadcastFunc(inBroadcaster),
                    mPageSize(pageSize),
                    mAllocationZoneMapping(pageSize),
                    mPhysicalAllocations(inAllocator, inDeallocator),
                    mMemoryMappings(mAllocationZoneMapping, mPhysicalAllocations, inMapper, inUnmapper),
                    mMaxExcessBytes(maxExcessBytes),
                    mMaxMemoryViewAllocations(maxMemoryViewAllocations),
                    mTotalBytesOfPageDataHeld(0)
    {
    }

bool PageDataPlacements::byteIsMapped(BlockOrViewId mapping, int64_t byteOffset)
    {
    return mMemoryMappings.byteIsMapped(mapping, byteOffset);
    }

uint8_t* PageDataPlacements::getWriteableAddress(PhysicalMemoryAllocation allocation)
    {
    return mPhysicalAllocations.addressForAllocation(allocation);
    }

IntegerRange PageDataPlacements::pageContainingByteOffset(int64_t byteOffset)
    {
    int64_t block = int64_t(byteOffset / mPageSize);

    return IntegerRange(block * mPageSize, (block+1) * mPageSize);
    }

void PageDataPlacements::markThreadBlockedOn(BlockOrViewId object, int64_t byteOffset)
    {
    if (byteIsMapped(object, byteOffset))
        return;

    IntegerRange byteRangeInObject = pageContainingByteOffset(byteOffset);

    //find the zone object containing this byteoffset
    RangeToZoneMapping zoneMapping = mAllocationZoneMapping.zoneMappingFor(object, byteRangeInObject);

    LOG_DEBUG << "block on " << zoneMapping;

    mMemoryMappings.blockedThreadExists(object, zoneMapping);

    @match AllocationZone(zoneMapping.zoneContainingData())
        -| Block(id, offset) ->> {
            IntegerRange bytesInZone = zoneMapping.byteRangeInZone();

            //what's the biggest range we could actually populate if we wanted to?
            IntegerRange bytesInBlock = bytesInZone - offset;

            Nullable<IntegerRange> containingRange = mPageDataBytecountRangesDefined[id].rangeContaining(bytesInBlock.low());
            if (containingRange)
                {
                //clip the range to align with the zone block boundary
                containingRange = mAllocationZoneMapping.clipToPageSize(*containingRange + offset) - offset;

                if (containingRange->contains(bytesInBlock))
                    {
                    bytesInZone = *containingRange + offset;

                    zoneMapping = zoneMapping.expandedToZoneRange(bytesInZone);

                    lassert(zoneMapping.byteRangeInZone().low() >= 0);
                    lassert(mAllocationZoneMapping.clipToPageSize(zoneMapping.byteRangeInZone()) == zoneMapping.byteRangeInZone());
                    }
                }
            }
        -| _ ->> {}

    tryToConstructZone(zoneMapping.zoneContainingData(), zoneMapping.byteRangeInZone());

    releaseMemoryIfNecessary();
    }

void PageDataPlacements::tryToConstructZone(AllocationZone zone, IntegerRange byteRangeInZone)
    {
    std::vector<ZoneToBlockMapping> zoneMappings;

    IntegerRange paddingZeros;
    mAllocationZoneMapping.mapZoneRangeToBlockRanges(zone, byteRangeInZone, zoneMappings, paddingZeros);

    std::vector<pair<MemoryBlockId, IntegerRange> > missingPageBytecountRanges;

    for (auto mapping: zoneMappings)
        {
        std::vector<IntegerRange> byteRangesOfThisPageNeeded;

        mPageDataBytecountRangesDefined[mapping.block()].rangesNotCovered(mapping.byteRangeInBlock(), byteRangesOfThisPageNeeded);

        for (auto range: byteRangesOfThisPageNeeded)
            missingPageBytecountRanges.push_back(make_pair(mapping.block(), range));

        std::vector<IntegerRange> byteRangesOfThisPageToRequest;

        mPageDataBytecountRangesDefinedOrRequested[mapping.block()].rangesNotCovered(mapping.byteRangeInBlock(), byteRangesOfThisPageToRequest);

        for (auto range: byteRangesOfThisPageToRequest)
            mBroadcastFunc(mapping.block(), range);
        }

    if (!missingPageBytecountRanges.size())
        scheduleNecessaryTasksForMapping(zone, byteRangeInZone);
    else
        mMemoryMappings.zoneNeeds(zone, byteRangeInZone, missingPageBytecountRanges);
    }

void PageDataPlacements::scheduleNecessaryTasksForMapping(AllocationZone zone, IntegerRange byteRangeInZone)
    {
    //ensure we have physical backing for the whole zone
    std::vector<IntegerRange> coveringRanges;
    std::vector<IntegerRange> notCovered;

    mPhysicalAllocations.allocationsForZone(zone)
        .subdivide(byteRangeInZone, coveringRanges, notCovered);

    for (auto range: notCovered)
        {
        PhysicalMemoryAllocation alloc(zone, range);

        mPhysicalAllocations.allocatePhysicalMemory(alloc);

        //get the region of this zone that may contain undefined memory
        IntegerRange padding = mAllocationZoneMapping.zeroPaddingBytesFor(alloc);

        if (padding.size())
            {
            mZoneBytecountRangesPopulated[zone].addRange(padding + alloc.byteRange().low());
            mZoneBytecountRangesPopulatedOrPending[zone].addRange(padding + alloc.byteRange().low());

            mPhysicalMemoryBytecountRangesPopulated[alloc].addRange(padding);
            }
        }

    std::vector<IntegerRange> neededBytecountRanges;

    mZoneBytecountRangesPopulatedOrPending[zone].rangesNotCovered(byteRangeInZone, neededBytecountRanges);

    for (auto neededBytesInZone: neededBytecountRanges)
        {
        IntegerRange paddingZeros;

        //these are regions we need to copy from somewhere else
        std::vector<ZoneToBlockMapping> neededPageRanges;

        mAllocationZoneMapping.mapZoneRangeToBlockRanges(
            zone,
            neededBytesInZone,
            neededPageRanges,
            paddingZeros
            );

        for (auto zoneBlockMapping: neededPageRanges)
            scheduleCopyTask(zone, zoneBlockMapping);
        }
    }

void PageDataPlacements::scheduleCopyTask(AllocationZone zone, ZoneToBlockMapping zoneBlockMapping)
    {
    //break the zone mappings into chunks
    std::vector<IntegerRange> allocatedRanges;
    std::vector<IntegerRange> notCovered;

    mPhysicalAllocations.allocationsForZone(zone).subdivide(
        zoneBlockMapping.byteRangeInZone(),
        allocatedRanges,
        notCovered
        );

    lassert(notCovered.size() == 0);

    for (auto physicalRange: allocatedRanges)
        {
        IntegerRange bytesInZone = zoneBlockMapping.byteRangeInZone().intersect(physicalRange);

        //now restrict to bytes that are not already populated
        std::vector<IntegerRange> bytesInZoneNotPopulated;
        mZoneBytecountRangesPopulated[zone].rangesNotCovered(bytesInZone, bytesInZoneNotPopulated);

        for (auto notPopulated: bytesInZoneNotPopulated)
            {
            mZoneBytecountRangesPopulatedOrPending[zone].addRange(notPopulated);

            IntegerRange bytesInPhysicalRange = notPopulated - physicalRange.low();

            IntegerRange bytesInBlock = zoneBlockMapping.mapZoneSubrangeToBlock(notPopulated);

            scheduleCopyTask(
                PhysicalMemoryAllocation(
                    zone,
                    physicalRange
                    ),
                AllocationToBlockMapping(
                    bytesInPhysicalRange,
                    zoneBlockMapping.block(),
                    bytesInBlock
                    )
                );
            }
        }
    }

void PageDataPlacements::scheduleCopyTask(PhysicalMemoryAllocation destAlloc, AllocationToBlockMapping allocationPageMapping)
    {
    while (allocationPageMapping.byteRangeInBlock().size())
        {
        bool foundAnything = false;
        for (auto possibleZoneAndRefcount: mAllocationZoneMapping.zonesActiveForBlock(allocationPageMapping.block()))
            {
            AllocationZone possibleZone = possibleZoneAndRefcount.first;

            lassert(possibleZone.isBlock());

            IntegerRange desiredPageBytesInThisZone = allocationPageMapping.byteRangeInBlock() + possibleZone.getBlock().byteOffset();

            //find the physical range containing this data if it exists
            Nullable<IntegerRange> zoneRange =
                mPhysicalAllocations.allocationsForZone(possibleZone).rangeContaining(desiredPageBytesInThisZone.low());

            if (zoneRange)
                {
                PhysicalMemoryAllocation sourceAlloc(possibleZone, *zoneRange);

                IntegerRange bytesInPhysicalAlloc = desiredPageBytesInThisZone.intersect(*zoneRange) - zoneRange->low();

                Nullable<IntegerRange> definedBytesWithin =
                    mPhysicalMemoryBytecountRangesPopulated[sourceAlloc].rangeContaining(bytesInPhysicalAlloc.low());

                if (definedBytesWithin)
                    {
                    IntegerRange bytesToCopyFromPhysicalZone = definedBytesWithin->intersect(bytesInPhysicalAlloc);

                    IntegerRange bytesToCopyTo = allocationPageMapping.byteRangeInAlloc();
                    lassert(bytesToCopyFromPhysicalZone.size() <= bytesToCopyTo.size());
                    bytesToCopyTo.high() = bytesToCopyTo.low() + bytesToCopyFromPhysicalZone.size();

                    lassert_dump(
                        bytesToCopyTo.size() > 0,
                        prettyPrintString(bytesInPhysicalAlloc) << " contained in " << prettyPrintString(definedBytesWithin)
                        );

                    scheduleTask(
                        DataCopyTask(
                            mPhysicalAllocations.addressForAllocation(destAlloc) + bytesToCopyTo.low(),
                            mPhysicalAllocations.addressForAllocation(sourceAlloc) + bytesToCopyFromPhysicalZone.low(),
                            null() << destAlloc,
                            null() << sourceAlloc,
                            bytesToCopyTo.size()
                            )
                        );

                    allocationPageMapping.byteRangeInBlock().low() += bytesToCopyTo.size();
                    allocationPageMapping.byteRangeInAlloc().low() += bytesToCopyTo.size();

                    if (!allocationPageMapping.byteRangeInBlock().size())
                        return;

                    foundAnything = true;
                    }
                }
            }

        lassert_dump(foundAnything, "couldn't find anything for " << allocationPageMapping.byteRangeInBlock().low() << " within "
                << prettyPrintString(allocationPageMapping.block()));
        }
    }

void PageDataPlacements::taskComplete(DataCopyTask task)
    {
    mPhysicalAllocations.copyAddressRefcountChange(task.dest(), task.destAlloc(), -1);
    mPhysicalAllocations.copyAddressRefcountChange(task.source(), task.sourceAlloc(), -1);

    uint8_t* destLow = task.dest();
    int64_t bytesPopulated = task.bytecount();

    lassert(bytesPopulated > 0);

    pair<PhysicalMemoryAllocation, int64_t> allocAndOffset = mPhysicalAllocations.allocContaining(destLow);

    PhysicalMemoryAllocation alloc = allocAndOffset.first;

    int64_t offsetInAlloc = allocAndOffset.second;

    IntegerRange allocBytesPopulated(
        offsetInAlloc,
        offsetInAlloc + bytesPopulated
        );

    IntegerRange zoneBytesPopulated = allocBytesPopulated + alloc.byteRange().low();

    LOG_DEBUG << "populated " << zoneBytesPopulated << " in " << alloc;

    if (alloc.zone().isBlock())
        {
        MemoryBlockId block = alloc.zone().getBlock().block();

        IntegerRange pageBytesAllocated = zoneBytesPopulated - alloc.zone().getBlock().byteOffset();

        mPageDataBytecountRangesDefined[block].addRange(pageBytesAllocated);

        mTotalBytesOfPageDataHeld += mPageDataBytecountRangesDefined[block].total() - mPerPageBytesHeld[block];
        mPerPageBytesHeld[block] = mPageDataBytecountRangesDefined[block].total();

        mPageDataBytecountRangesDefinedOrRequested[block].addRange(pageBytesAllocated);
        mMemoryMappings.blockDataProvided(block, pageBytesAllocated);
        }

    mPhysicalMemoryBytecountRangesPopulated[alloc].addRange(allocBytesPopulated);

    mZoneBytecountRangesPopulated[alloc.zone()].addRange(allocBytesPopulated + alloc.byteRange().low());
    mZoneBytecountRangesPopulatedOrPending[alloc.zone()].addRange(allocBytesPopulated + alloc.byteRange().low());

    //the range that bytesPopulated is contained in is now a candidate for mapping
    IntegerRange containingRange = *mPhysicalMemoryBytecountRangesPopulated[alloc].rangeContaining(allocBytesPopulated.low());

    //limit this to block size
    containingRange = mAllocationZoneMapping.clipToPageSize(containingRange);

    if (containingRange.size())
        {
        LOG_DEBUG << alloc.zone() << " is now mappable: " << containingRange + alloc.byteRange().low();

        mMemoryMappings.zoneIsNowMappable(alloc.zone(), containingRange + alloc.byteRange().low());
        }

    std::vector<pair<AllocationZone, IntegerRange> > zonesToTryToMap;
    mMemoryMappings.extractZonesToTryToMap(zonesToTryToMap);

    for (auto z: zonesToTryToMap)
        tryToConstructZone(z.first, z.second);

    releaseMemoryIfNecessary();
    }

void PageDataPlacements::releaseMemoryIfNecessary()
    {
    releaseMemoryViewsIfNecessary();
    consolidatePagesIfNecessary();
    releasePagesIfNecessary();
    }

void PageDataPlacements::releasePagesIfNecessary()
    {
    std::set<MemoryBlockId> newBlocksToDeallocate;

    auto canDeallocatePage = [&](MemoryBlockId block) {
        if (mPhysicalAllocations.blockHasCopyTasks(block))
            return false;

        for (auto allocationZoneAndRefcount: mAllocationZoneMapping.zonesActiveForBlock(block))
            if (mMemoryMappings.zoneHasPendingMappings(allocationZoneAndRefcount.first))
                return false;

        return true;
        };

    for (auto id: mPagesToDeallocate)
        if (canDeallocatePage(id))
            completelyRemovePage(id);
        else
            newBlocksToDeallocate.insert(id);

    std::swap(newBlocksToDeallocate, mPagesToDeallocate);
    }

void PageDataPlacements::consolidatePagesIfNecessary()
    {
    while (mPhysicalAllocations.totalBytesMapped() >
                mTotalBytesOfPageDataHeld + mMaxExcessBytes +
                //don't count the memory view blocks in the memory count - we don't consider
                //them in the compression count.
                mPhysicalAllocations.countOfMemoryViewAllocs() * mPageSize)
        {
        Nullable<MemoryBlockId> block = mPhysicalAllocations.getOldestBlock();
        if (!block)
            return;

        if (mPhysicalAllocations.blockHasCopyTasks(*block))
            return;

        Largest<AllocationZone, int64_t> zones;

        //pick the allocation zone with the most data populated in it - that'll be our target block
        //if any of these zones have pending mappings, we can't consolidate
        for (auto allocationZoneAndRefcount: mAllocationZoneMapping.zonesActiveForBlock(*block))
            {
            if (mMemoryMappings.zoneHasPendingMappings(allocationZoneAndRefcount.first))
                return;

            zones.observe(
                allocationZoneAndRefcount.first,
                mZoneBytecountRangesPopulated[allocationZoneAndRefcount.first].total()
                );
            }


        //schedule copy tasks moving all populated data into the populated zone
        if (!zones.largest())
            {
            //this block is dead
            mPhysicalAllocations.removeBlockFromReplacementQueue(*block);
            }
        else
            {
            AllocationZone targetZone = *zones.largest();

            bool scheduledAnything = false;

            for (auto allocationZoneAndRefcount: mAllocationZoneMapping.zonesActiveForBlock(*block))
                if (allocationZoneAndRefcount.first != targetZone)
                    if (scheduleCopyAllDataBetweenZones(allocationZoneAndRefcount.first, targetZone))
                        scheduledAnything = true;

            if (scheduledAnything)
                return;

            //we can remove all data for all zones other than the one we want
            for (auto allocationZoneAndRefcount: mAllocationZoneMapping.zonesActiveForBlock(*block))
                if (allocationZoneAndRefcount.first != targetZone)
                    completelyRemoveZone(allocationZoneAndRefcount.first);

            mPhysicalAllocations.removeBlockFromReplacementQueue(*block);
            }
        }
    }

bool PageDataPlacements::scheduleCopyAllDataBetweenZones(AllocationZone sourceZone, AllocationZone destZone)
    {
    lassert(sourceZone.isBlock());
    lassert(destZone.isBlock());

    int64_t sourceToDestOffset = destZone.getBlock().byteOffset() - sourceZone.getBlock().byteOffset();

    IntegerRanges<true> rangesToPopulate;

    for (auto startAndEnd: mZoneBytecountRangesPopulated[sourceZone].getRangeStartsAndEnds())
        {
        IntegerRange range(startAndEnd.first, startAndEnd.second);

        std::vector<IntegerRange> notCoveredInDest;

        mZoneBytecountRangesPopulatedOrPending[destZone].rangesNotCovered(range + sourceToDestOffset, notCoveredInDest);

        for (auto rng: notCoveredInDest)
            rangesToPopulate.addRange(expandRangeToPageSize(rng));
        }

    if (rangesToPopulate.isEmpty())
        return false;

    for (auto startAndEnd: rangesToPopulate.getRangeStartsAndEnds())
        {
        IntegerRange range(startAndEnd.first, startAndEnd.second);

        std::vector<IntegerRange> notCoveredInDestPhysically;

        //restrict to a set of ranges that cover what we need.
        mPhysicalAllocations.allocationsForZone(destZone)
            .rangesNotCovered(range, notCoveredInDestPhysically);

        //each one becomes a new physical allocation we need to make
        for (auto rng: notCoveredInDestPhysically)
            mPhysicalAllocations.allocatePhysicalMemory(
                PhysicalMemoryAllocation(
                    destZone,
                    rng
                    )
                );
        }

    bool scheduledAnything = false;

    for (auto startAndEnd: mZoneBytecountRangesPopulated[sourceZone].getRangeStartsAndEnds())
        {
        IntegerRange range(startAndEnd.first, startAndEnd.second);

        std::vector<IntegerRange> notCoveredInDest;

        mZoneBytecountRangesPopulatedOrPending[destZone].rangesNotCovered(range + sourceToDestOffset, notCoveredInDest);

        for (auto rng: notCoveredInDest)
            {
            scheduledAnything = true;
            scheduleCopyTask(
                destZone,
                ZoneToBlockMapping(rng, sourceZone.getBlock().block(), rng - destZone.getBlock().byteOffset())
                );

            lassert_dump(
                mZoneBytecountRangesPopulatedOrPending[destZone].completelyContainsRange(rng),
                "Added " << prettyPrintString(rng) << " now having "
                    << prettyPrintString(mZoneBytecountRangesPopulatedOrPending[destZone].getRangeStartsAndEnds())
                    << ". populated= " << prettyPrintString(mZoneBytecountRangesPopulated[destZone].getRangeStartsAndEnds())
                );
            }
        }

    return scheduledAnything;
    }

void PageDataPlacements::releaseMemoryViewsIfNecessary()
    {
    while (mPhysicalAllocations.countOfMemoryViewAllocs() > mMaxMemoryViewAllocations)
        {
        Nullable<PhysicalMemoryAllocation> alloc = mPhysicalAllocations.oldestMemoryViewAlloc();

        lassert(alloc);

        //we have to stop and wait for the copy task to finish
        if (mPhysicalAllocations.allocHasCopyTasks(*alloc) || mMemoryMappings.zoneHasPendingMappings(alloc->zone()))
            return;

        lassert(!mMemoryMappings.hasZonesToTryToMap());

        //we can safely delete this bigvec
        completelyRemovePhysicalAlloc(*alloc);
        }
    }

void PageDataPlacements::completelyRemovePage(MemoryBlockId block)
    {
    for (auto allocationZoneAndRefcount: mAllocationZoneMapping.zonesActiveForBlock(block))
        completelyRemoveZone(allocationZoneAndRefcount.first);

    mTotalBytesOfPageDataHeld -= mPerPageBytesHeld[block];
    mPerPageBytesHeld[block] = 0;

    mPageDataBytecountRangesDefined.erase(block);
    mPageDataBytecountRangesDefinedOrRequested.erase(block);
    }

void PageDataPlacements::completelyRemoveZone(AllocationZone zone)
    {
    while (!mPhysicalAllocations.allocationsForZone(zone).isEmpty())
        {
        auto startAndEnd = *mPhysicalAllocations.allocationsForZone(zone).getRangeStartsAndEnds().begin();
        IntegerRange physicalRange(startAndEnd.first, startAndEnd.second);

        completelyRemovePhysicalAlloc(PhysicalMemoryAllocation(zone, physicalRange));
        }
    }

void PageDataPlacements::completelyRemovePhysicalAlloc(PhysicalMemoryAllocation alloc)
    {
    lassert(!mPhysicalAllocations.allocHasCopyTasks(alloc));

    mMemoryMappings.removeAllMappingsFor(alloc);

    //now deallocate it
    mPhysicalAllocations.deallocatePhysicalMemory(alloc);
    mPhysicalMemoryBytecountRangesPopulated.erase(alloc);
    mZoneBytecountRangesPopulated[alloc.zone()].removeRange(alloc.byteRange());
    mZoneBytecountRangesPopulatedOrPending[alloc.zone()].removeRange(alloc.byteRange());

    mMemoryMappings.deallocatePhysicalMemory(alloc);
    }

void PageDataPlacements::scheduleTask(DataCopyTask task)
    {
    lassert(task.destAlloc() != task.sourceAlloc());

    lassert(task.bytecount() > 0);
    mPhysicalAllocations.copyAddressRefcountChange(task.dest(), task.destAlloc(), 1);
    mPhysicalAllocations.copyAddressRefcountChange(task.source(), task.sourceAlloc(), 1);
    mTasks.push_back(task);
    }

Nullable<pair<BlockOrViewId, int64_t> >
        PageDataPlacements::translateMappingAddress(uint8_t* mappingBase, uint64_t offsetInMapping)
    {
    return mPhysicalAllocations.translateMappingAddress(mappingBase, offsetInMapping);
    }

void PageDataPlacements::addBigVecPageLayout(const MemoryViewDefinition& layout)
    {
    mAllocationZoneMapping.addMemoryView(layout);
    }

void PageDataPlacements::setMappableAddress(
            BlockOrViewId target,
            uint8_t* address,
            uint64_t bytecount
            )
    {
    mPhysicalAllocations.setMappableAddress(target, address, bytecount);
    }

uint8_t* PageDataPlacements::getMappableAddress(BlockOrViewId target)
    {
    return mPhysicalAllocations.getMappableAddress(target);
    }

void PageDataPlacements::dropMappingTarget(BlockOrViewId target)
    {
    mPhysicalAllocations.dropMappingTarget(target);

    if (target.isView())
        mAllocationZoneMapping.discardMemoryView(target.getView().view());
    }

IntegerRange PageDataPlacements::expandRangeToPageSize(IntegerRange byteRange)
    {
    //compute the block blocks we'll need to store this data. These are inclusive - we have to include the top byte
    //of the topmost item
    int64_t lowPageBlock = byteRange.low() / mPageSize;
    int64_t highPageBlock = (byteRange.high() - 1) / mPageSize;

    return IntegerRange(
        lowPageBlock * mPageSize,
        (highPageBlock + 1) * mPageSize
        );
    }

void PageDataPlacements::ensurePhysicalBackingFor(
            const AllocationZone& zone,
            IntegerRange byteRange
            )
    {
    std::vector<IntegerRange> subrangesNotDefined;

    //restrict to a set of ranges that cover what we need.
    mPhysicalAllocations.allocationsForZone(zone)
        .rangesNotCovered(byteRange, subrangesNotDefined);

    //each one becomes a new physical allocation we need to make
    for (auto rng: subrangesNotDefined)
        mPhysicalAllocations.allocatePhysicalMemory(
            PhysicalMemoryAllocation(
                zone,
                rng
                )
            );
    }

bool PageDataPlacements::allocatePageData(
                const MemoryBlockId& inPage,
                IntegerRange incomingByteRange,
                uint8_t* addr
                )
    {
    lassert_dump(incomingByteRange.size(), "makes no sense to allocate empty block data");

    IntegerRange hostingRange = expandRangeToPageSize(incomingByteRange);

    AllocationZone zone = AllocationZone::Block(inPage, 0);

    ensurePhysicalBackingFor(zone, hostingRange);

    std::vector<IntegerRange> subrangesNotDefined;

    mPageDataBytecountRangesDefined[inPage].rangesNotCovered(incomingByteRange, subrangesNotDefined);

    for (auto byteRange: subrangesNotDefined)
        {
        std::vector<IntegerRange> subrangesCovering, partsNotCovered;

        //each range here should be contained within one of the physical ranges we have defined for the block
        IntegerRanges<false>& sequences = mPhysicalAllocations.allocationsForZone(zone);

        sequences.subdivide(byteRange, subrangesCovering, partsNotCovered);

        lassert(partsNotCovered.size() == 0);

        for (auto physicalBytecountRange: subrangesCovering)
            {
            IntegerRange intersection = physicalBytecountRange.intersect(byteRange);

            PhysicalMemoryAllocation alloc(
                zone,
                physicalBytecountRange
                );

            uint8_t* allocAddr = mPhysicalAllocations.addressForAllocation(alloc);

            lassert_dump(
                intersection.size() > 0,
                prettyPrintString(physicalBytecountRange) << " within " << prettyPrintString(byteRange)
                );

            scheduleTask(
                DataCopyTask(
                    allocAddr + intersection.low() - physicalBytecountRange.low(),
                    addr + intersection.low() - incomingByteRange.low(),
                    null() << alloc,
                    null(),
                    intersection.size()
                    )
                );
            }
        }

    return true;
    }

void PageDataPlacements::dropAllDataForPage(const MemoryBlockId& inBlock)
    {
    mPagesToDeallocate.insert(inBlock);
    releaseMemoryIfNecessary();
    }

Nullable<DataCopyTask> PageDataPlacements::extractTask()
    {
    if (mTasks.size())
        {
        DataCopyTask t = *mTasks.begin();
        mTasks.pop_front();

        if (t.destAlloc())
            {
            lassert(mPhysicalAllocations.allocHasCopyTasks(*t.destAlloc()));
            lassert(mPhysicalAllocations.addressForAllocation(*t.destAlloc()));
            }

        if (t.sourceAlloc())
            {
            lassert(mPhysicalAllocations.allocHasCopyTasks(*t.sourceAlloc()));
            lassert(mPhysicalAllocations.addressForAllocation(*t.sourceAlloc()));
            }

        return null() << t;
        }

    return null();
    }

int64_t PageDataPlacements::bytesOfPageDataHeld() const
    {
    return mTotalBytesOfPageDataHeld;
    }

int64_t PageDataPlacements::bytesOfPageDataHeld(MemoryBlockId block) const
    {
    auto it = mPerPageBytesHeld.find(block);

    if (it == mPerPageBytesHeld.end())
        return 0;

    return it->second;
    }

int64_t PageDataPlacements::totalBytesOfPhysicalMemoryAllocated() const
    {
    return mPhysicalAllocations.totalBytesMapped();
    }

void PageDataPlacements::getPageDataDefinedFor(MemoryBlockId block, IntegerRanges<true>& outRanges)
    {
    outRanges = mPageDataBytecountRangesDefined[block];
    }
