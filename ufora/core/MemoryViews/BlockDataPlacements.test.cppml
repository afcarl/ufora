/***************************************************************************
    Copyright 2016 Ufora Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/

#include "MemoryViewDefinition.hppml"
#include "BlockDataPlacements.hppml"
#include "OnDemandMemoryRegions.hppml"

#include "../../core/UnitTest.hpp"
#include "../../core/UnitTestCppml.hpp"
#include "../../core/Logging.hpp"
#include "../../core/threading/Queue.hpp"
#include "../../core/math/Random.hpp"
#include "../../core/containers/ImmutableTreeVector.hppml"

class BlockDataPlacementsTest {
public:
    BlockDataPlacementsTest() :
            mMemoryRegions(
                [&](OnDemandMemoryRegions::BlockingThread t) {
                    LOG_CRITICAL << "this should never happen.";
                    fflush(stdout);
                    fflush(stderr);
                    _exit(1);
                    },
                []() -> void* { return (void*)1; },
                "/ufora"
                ),
            mPageSize(mMemoryRegions.pageSize()),
            mPlacements(
                mPageSize,
                [&](PhysicalMemoryAllocation alloc) {
                    mAllocOps++;
                    return mMemoryRegions.allocateSharedRegion(alloc.byteRange().size());
                    },
                [&](PhysicalMemoryAllocation dealloc, uint8_t* ptr) {
                    mMemoryRegions.freeSharedRegion(ptr);
                    },
                [&](uint8_t* sharedBase, uint64_t sharedOffset, uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount) {
                    mMapOps++;
                    return mMemoryRegions.mapShareableRegion(sharedBase, sharedOffset, mapBase, mapOffset, bytecount);
                    },
                [&](uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount) {
                    mMapOps++;
                    return mMemoryRegions.unmap(mapBase, mapOffset, bytecount);
                    },
                [&](MemoryBlockId block, IntegerRange range) {
                    mNeededBlockRegions.write(make_pair(block, range));
                    mBlockUploadOps++;
                    },
                1024 * 1024,
                32
                ),
            mBytesPerBlock(1024 * 128),
            mRandom(1),
            mCopyBytes(0),
            mCopyOps(0),
            mMapOps(0),
            mAllocOps(0),
            mBlockUploadOps(0)
        {
        allocateBlocks(10);
        allocateBigvecs(20);
        }

    MemoryBlockId testBlock(int i)
        {
        return MemoryBlockId(hash_type(i));
        }

    void allocateBlocks(int64_t count)
        {
        int64_t value = 0;

        for (int64_t k = 0; k < count;k++)
            {
            mBlocks.push_back(testBlock(k));
            for (int64_t k = 0; k < mBytesPerBlock / sizeof(int64_t)+1;k++)
                mBlockDatas[mBlocks.back()].push_back(value++);
            }
        }

    void allocateBigvecs(int64_t viewCount)
        {
        for (long k = 0; k < viewCount; k++)
            {
            ImmutableTreeVector<MemoryViewDefinitionTerm> terms;

            long blockCount = 2 + mRandom() * 10;

            ImmutableTreeVector<pair<int64_t, IntegerRange> > counts;

            uint64_t bytecount = 0;

            for (long b = 0; b < blockCount; b++)
                {
                int64_t width = mPageSize * (1 + int(mRandom() * 14));
                int64_t offset = mPageSize * int(mRandom() * 14);

                if (mRandom() < .25)
                    {
                    //let's make the block width weird
                    if (mRandom() < .1)
                        //really weird
                        width -= mRandom() * 125;
                    else
                        //just a little weird
                        width -= mPageSize / 2;
                    }

                if (mRandom() < .5)
                    offset += mRandom() * 512;

                lassert(offset+width < mBytesPerBlock);

                terms = terms + 
                    MemoryViewDefinitionTerm::ViewOfBlock(
                        pickRandomBlock(),
                        IntegerRange(offset, offset+width)
                        );

                bytecount += width;
                }

            MemoryViewId viewId = hashValue(terms);

            MemoryViewDefinition viewDefinition(viewId, terms);

            mViewDefinitions[viewId] = viewDefinition;
            mViewBytecountMapping[viewId] = bytecount;
            mViews.push_back(viewId);
            }
        }

    void touchRandomly(long count)
        {
        for (long k = 0; k < count; k++)
            {
            if (mRandom() < 0.0001)
                dropARandomBlock();

            touchARandomBlock();
            }

        int64_t totalBytes = 0;
        for (auto& blockAndData: mBlockDatas)
            totalBytes += blockAndData.second.size() * sizeof(int64_t);

        LOG_INFO << "after " << count << " value touches:\n"
            << "MB copied: " << mCopyBytes / 1024 / 1024.0 << "\n"
            << "Copy operations: " << mCopyOps << "\n"
            << "Bytes/copy operation: " << mCopyBytes / mCopyOps << "\n"
            << "Map ops: " << mMapOps << "\n"
            << "Alloc ops: " << mAllocOps << "\n"
            << "block upload ops: " << mBlockUploadOps << "\n"
            << "Total mb of block data: " << totalBytes / 1024 / 1024.0
            ;
        }

    void dropARandomBlock()
        {
        MemoryBlockId block = pickRandomBlock();

        int64_t bytesHeldForBlock = mPlacements.bytesOfBlockDataHeld(block);
        int64_t bytesHeldTotal = mPlacements.totalBytesOfBlockDataHeld();

        mPlacements.dropAllDataForBlock(block);

        lassert(mPlacements.bytesOfBlockDataHeld(block) == 0);
        lassert(bytesHeldTotal - mPlacements.totalBytesOfBlockDataHeld() == bytesHeldForBlock);
        }

    void touchARandomBlock()
        {
        BlockOrViewId ent = pickRandomEntity();
        int64_t bytecount;

        if (ent.isBlock())
            bytecount = mBytesPerBlock;
        else
            bytecount = mViewBytecountMapping[ent.getView().view()];

        int64_t aRandomByte = mRandom() * bytecount;

        lassert(aRandomByte < bytecount);

        LOG_DEBUG << "Checking byte " << aRandomByte << " in " << bytecount << " of " << ent;

        if (!mPlacements.getMappableAddress(ent))
            {
            if (ent.isView())
                mPlacements.addMemoryViewDefinition(mViewDefinitions[ent.getView().view()]);

            mPlacements.setMappableAddress(ent, mMemoryRegions.allocateMappableRegion(bytecount), bytecount);
            }

        bool everCured = false;

        //we are not guaranteed that the byte is still mapped after we run all the update tasks,
        //since we could GC in the middle. Instead, we are only guaranteed that if the data
        //is available, that at some point after calling 'markThreadBlockedOn' we will eventually
        //see the byte available.
        if (!mPlacements.byteIsMapped(ent, aRandomByte))
            mPlacements.markThreadBlockedOn(ent, aRandomByte);
        else
            everCured = true;

        do {
            if (mPlacements.byteIsMapped(ent, aRandomByte))
                {
                everCured = true;

                uint8_t byte = mPlacements.getMappableAddress(ent)[aRandomByte];

                lassert_dump(byte == computeByteForEntity(ent, aRandomByte),
                    "expected " << prettyPrintString(ent) << " byte " << aRandomByte
                        << " to be " << (long)computeByteForEntity(ent, aRandomByte) << " but got " << (long)byte
                        << "\n"
                        );
                }
            } while (updateStep());

        lassert_dump(
            everCured,
            "We never actually cured the fault."
            );
        }

    uint8_t computeByteForEntity(BlockOrViewId ent, int64_t byte)
        {
        @match BlockOrViewId(ent)
            -| Block(id) ->> {
                lassert_dump(
                    mBlockDatas[id].size() * sizeof(int64_t) > byte,
                    "byte " << byte << " greater than size " << mBlockDatas[id].size() * sizeof(int64_t)
                    );
                return ((uint8_t*)&mBlockDatas[id][0])[byte];
                }
            -| View(id) ->> {
                auto blockTermAndOffset = mViewDefinitions[id].blockTermAndOffsetContainingIndex(byte);

                int64_t offsetInBlock = byte - blockTermAndOffset.second;

                return computeByteForEntity(
                    BlockOrViewId::Block(blockTermAndOffset.first.getViewOfBlock().block()),
                    blockTermAndOffset.first.byterange().low() + offsetInBlock
                    );
                }
        }

    MemoryBlockId pickRandomBlock()
        {
        return mBlocks[mRandom() * mBlocks.size()];
        }

    MemoryViewId pickRandomView()
        {
        return mViews[mRandom() * mViews.size()];
        }
    BlockOrViewId pickRandomEntity()
        {
        return (mRandom() < .5) ?
            BlockOrViewId::Block(pickRandomBlock()) :
            BlockOrViewId::View(pickRandomView());
        }

    bool updateStep()
        {
        lassert_dump(
            mPlacements.totalBytesOfPhysicalMemoryAllocated() - mPlacements.totalBytesOfBlockDataHeld() < 1024 * 1024 * 2,
            mPlacements.totalBytesOfPhysicalMemoryAllocated() / 1024 / 1024.0 << " MB of physical data. "
                << mPlacements.totalBytesOfBlockDataHeld() / 1024 / 1024.0 << " MB of block data. excess = "
                << (mPlacements.totalBytesOfPhysicalMemoryAllocated() - mPlacements.totalBytesOfBlockDataHeld()) / 1024 / 1024.0
                << " MB."
            );

        if (auto task = mPlacements.extractTask())
            {
            mCopyOps++;
            mCopyBytes += task->bytecount();

            task->copy();
            mPlacements.taskComplete(*task);

            return true;
            }

        if (auto needed = mNeededBlockRegions.getNonblock())
            {
            int64_t index = 0;

            int64_t blockBytes = mBlockDatas[needed->first].size() * sizeof(int64_t);

            lassert_dump(
                needed->second.high() <= mBlockDatas[needed->first].size() * sizeof(int64_t),
                "Asked for " << prettyPrintString(needed) << " but we only have " << blockBytes
                );

            //sometimes, we give more than we need
            if (mRandom() < .2)
                {
                if (mRandom() < .1)
                    needed->second = IntegerRange(0, blockBytes);
                else
                    {
                    needed->second.low() = std::max<int64_t>(0, needed->second.low() - mRandom() * mPageSize);
                    needed->second.high() = std::min<int64_t>(blockBytes, needed->second.high() + mRandom() * mPageSize);
                    }
                }

            LOG_DEBUG << "Handing in " << needed;

            mPlacements.allocateBlockData(
                needed->first,
                needed->second,
                (uint8_t*)&mBlockDatas[needed->first][0] + needed->second.low()
                );

            return true;
            }

        return false;
        }

private:
    int64_t mCopyBytes;

    int64_t mCopyOps;

    int64_t mMapOps;

    int64_t mAllocOps;

    int64_t mBlockUploadOps;

    Ufora::math::Random::Uniform<double> mRandom;

    int64_t mBytesPerBlock;

    Queue<pair<MemoryBlockId, IntegerRange> > mNeededBlockRegions;

    OnDemandMemoryRegions mMemoryRegions;

    int64_t mPageSize;

    BlockDataPlacements mPlacements;

    std::vector<MemoryBlockId> mBlocks;

    std::map<MemoryBlockId, std::vector<int64_t> > mBlockDatas;

    std::map<MemoryViewId, MemoryViewDefinition> mViewDefinitions;

    std::vector<MemoryViewId> mViews;

    std::map<MemoryViewId, uint64_t> mViewBytecountMapping;
};


BOOST_AUTO_TEST_CASE( test_BlockDataPlacements_random )
	{
    BlockDataPlacementsTest test;

    test.touchRandomly(100000);
	}

namespace {

MemoryBlockId testBlock(int i)
    {
    return MemoryBlockId(hash_type(i));
    }

MemoryViewDefinition testLayout(ImmutableTreeVector<pair<int64_t, IntegerRange> > elts)
    {
    ImmutableTreeVector<MemoryViewDefinitionTerm> terms;

    for (auto kv: elts) 
        terms = terms +
            MemoryViewDefinitionTerm::ViewOfBlock(testBlock(kv.first), kv.second);


    return MemoryViewDefinition(hashValue(terms), terms);
    }

}

//walk through a sequence of very basic operations, just to make sure its working correctly.
BOOST_AUTO_TEST_CASE( test_BlockDataPlacements_basic )
    {
    Queue<pair<MemoryBlockId, IntegerRange> > needed;

    OnDemandMemoryRegions memoryRegions(
        //we designed the test suite so that we never actually trigger a page fault - we're trying to call
        //the functions on the BlockDataPlacements directly so that it doesn't happen. If this does happen, it means
        //that a mapping we expected to happen has not occurred.
        [&](OnDemandMemoryRegions::BlockingThread t) {
            LOG_CRITICAL << "this should never happen.";
            fflush(stdout);
            fflush(stderr);
            _exit(1);
            },
        []() -> void* { return (void*)1; },
        "/ufora"
        );

    int64_t pageSize = memoryRegions.pageSize();

    BlockDataPlacements placements(
        pageSize,
        [&](PhysicalMemoryAllocation alloc) {
            return memoryRegions.allocateSharedRegion(
                memoryRegions.roundUpToPageSize(alloc.byteRange().size())
                );
            },
        [&](PhysicalMemoryAllocation dealloc, uint8_t* ptr) {
            memoryRegions.freeSharedRegion(ptr);
            },
        [&](uint8_t* sharedBase, uint64_t sharedOffset, uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount) {
            return memoryRegions.mapShareableRegion(sharedBase, sharedOffset, mapBase, mapOffset, bytecount);
            },
        [&](uint8_t* mapBase, uint64_t mapOffset, uint64_t bytecount) {
            return memoryRegions.unmap(mapBase, mapOffset, bytecount);
            },
        [&](MemoryBlockId block, IntegerRange range) {
            needed.write(make_pair(block, range));
            },
        1024 * 1024 * 1024,
        1024
        );

    BlockOrViewId blockId = BlockOrViewId::Block(testBlock(0));

    placements.setMappableAddress(blockId, memoryRegions.allocateMappableRegion(pageSize), pageSize);

    uint8_t* pageAddress = placements.getMappableAddress(blockId);

    BOOST_CHECK(pageAddress != 0);

    BOOST_CHECK(!placements.byteIsMapped(blockId, 0));

    auto res = placements.translateMappingAddress(pageAddress, 1);

    BOOST_CHECK(res);
    BOOST_CHECK_EQUAL_CPPML(res->first, blockId);
    BOOST_CHECK_EQUAL(res->second, 1);


    placements.markThreadBlockedOn(blockId, 0);

    std::vector<uint8_t> someData;
    someData.resize(pageSize);
    someData[1] = 123;

    placements.allocateBlockData(blockId.getBlock().block(), IntegerRange(0, someData.size()), &someData[0]);

    auto task = placements.extractTask();
    BOOST_CHECK(task);

    BOOST_CHECK(task->source() == &someData[0]);
    BOOST_CHECK(task->bytecount() == someData.size());
    task->copy();

    placements.taskComplete(*task);
    BOOST_CHECK(!placements.extractTask());
    BOOST_CHECK(placements.byteIsMapped(blockId, 1));
    BOOST_CHECK(pageAddress[1] == 123);

    auto layout = testLayout(emptyTreeVec() + make_pair(int64_t(0), IntegerRange(0, pageSize)) + make_pair(int64_t(1), IntegerRange(1,pageSize+1)));

    BlockOrViewId vecId(BlockOrViewId::View(layout.viewId()));

    placements.addMemoryViewDefinition(layout);

    placements.setMappableAddress(vecId, memoryRegions.allocateMappableRegion(pageSize * 2), pageSize * 2);

    uint8_t* bvAddr = placements.getMappableAddress(vecId);

    placements.markThreadBlockedOn(vecId, 1);
    BOOST_CHECK(placements.byteIsMapped(vecId, 1));

    //nothing should need to be copied
    BOOST_CHECK(bvAddr[1] == 123);

    //define the next block
    someData.resize(pageSize * 2);
    someData[1] = 23;
    placements.allocateBlockData(testBlock(1), IntegerRange(0, someData.size() * 2), &someData[0]);

    placements.markThreadBlockedOn(vecId, pageSize);

    while (auto task = placements.extractTask())
        {
        task->copy();
        placements.taskComplete(*task);
        }

    BOOST_CHECK(placements.byteIsMapped(vecId, pageSize));

    BOOST_CHECK_EQUAL((long)bvAddr[pageSize], (long)23);
    }





